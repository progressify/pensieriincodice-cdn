1
00:00:00,000 --> 00:00:01,490
Dilla verità,

2
00:00:01,490 --> 00:00:05,468
anche tu qualche volta hai utilizzato un chatbot o un motore

3
00:00:05,468 --> 00:00:09,446
di ricerca basato su intelligenza artificiale generativa per

4
00:00:09,446 --> 00:00:13,489
farti dare delle risposte o dei consigli su qualche argomento?

5
00:00:13,490 --> 00:00:20,059
Magari in un ambito nel quale non hai troppa
dimestichezza o senti il bisogno di una guida.

6
00:00:20,059 --> 00:00:21,489
Non c'è niente di male,

7
00:00:21,489 --> 00:00:24,280
ovviamente lo facciamo tutti io per primo.

8
00:00:24,280 --> 00:00:30,035
Ma c'è una cosa da tenere bene a mente quando utilizziamo un large language

9
00:00:30,035 --> 00:00:35,487
model e cioè che esso non solo potrebbe facilmente commettere errori ma

10
00:00:35,487 --> 00:00:39,955
potrebbe anche mentire o addirittura assecondare le nostre

11
00:00:39,955 --> 00:00:44,119
affermazioni anche se queste sono completamente errate.

12
00:00:44,119 --> 00:00:47,049
Oggi scopriamo insieme il motivo per cui,

13
00:00:47,049 --> 00:00:53,640
nonostante questi strumenti possano rappresentare
un enorme vantaggio per chi li utilizza,

14
00:00:53,640 --> 00:00:59,289
è bene sapere che non bisogna mai fidarsi ciecamente delle loro risposte.

15
00:00:59,289 --> 00:01:04,709
Parleremo infatti del perché le intelligenze artificiali inventano cose,

16
00:01:04,709 --> 00:01:07,400
ci assecondano e ci mentono.

17
00:01:07,400 --> 00:01:08,040
Sigla!

18
00:01:12,250 --> 00:01:14,329
Benvenuti su Pensieri in codice,

19
00:01:14,329 --> 00:01:16,250
il podcast dove si ragiona da informatici.

20
00:01:16,250 --> 00:01:18,250
Con Valerio Galano.

21
00:01:23,250 --> 00:01:25,709
Prima di iniziare con il nostro discorso,

22
00:01:25,709 --> 00:01:28,449
trovo doveroso fare alcuni chiarimenti.

23
00:01:28,450 --> 00:01:29,570
Innanzitutto,

24
00:01:29,569 --> 00:01:35,769
anche se in questo episodio utilizzeremo la
definizione generica di "intelligenza artificiale",

25
00:01:35,769 --> 00:01:41,879
ricordiamoci sempre che si parlerà solo ed
esclusivamente di machine learning generativo,

26
00:01:41,879 --> 00:01:44,609
nello specifico di large language model.

27
00:01:44,609 --> 00:01:46,069
In secondo luogo,

28
00:01:46,069 --> 00:01:48,839
e questo non lo dirò mai abbastanza,

29
00:01:48,840 --> 00:01:51,340
anche se utilizzeremo verbi come mentire,

30
00:01:51,340 --> 00:01:52,250
assecondare,

31
00:01:52,250 --> 00:01:53,049
decidere,

32
00:01:53,049 --> 00:01:53,849
imparare,

33
00:01:53,849 --> 00:01:54,719
eccetera,

34
00:01:54,719 --> 00:02:02,689
dobbiamo avere sempre ben chiaro in mente che gli LLM
non hanno alcuna intenzionalità nei loro comportamenti,

35
00:02:02,689 --> 00:02:07,289
né tantomeno una vera comprensione del significato dei testi che generano.

36
00:02:07,289 --> 00:02:09,750
Quando parliamo di allucinazioni,

37
00:02:09,750 --> 00:02:11,500
di pensiero o di menzogne,

38
00:02:11,500 --> 00:02:17,180
lo facciamo solo perché sono concetti che
ci risultano più facili da comprendere,

39
00:02:17,180 --> 00:02:23,569
e descrivono funzionamenti di queste macchine che
assomigliano ai relativi comportamenti tipici degli umani.

40
00:02:23,569 --> 00:02:26,919
Ma stiamo sempre parlando di macchine,

41
00:02:26,920 --> 00:02:28,250
non di umani.

42
00:02:28,250 --> 00:02:29,110
Pertanto,

43
00:02:29,110 --> 00:02:37,090
seppur apparendo in tutto e per tutto simili
all'interazione con un altro individuo della nostra specie,

44
00:02:37,090 --> 00:02:43,689
le ragioni dietro alla generazione di ogni
singola risposta sono completamente diverse.

45
00:02:43,689 --> 00:02:44,750
In realtà,

46
00:02:44,750 --> 00:02:47,340
lo ripetiamo ancora una volta,

47
00:02:47,340 --> 00:02:51,689
anche se ci ostiniamo a chiamarli "intelligenze",

48
00:02:51,689 --> 00:02:54,599
questi algoritmi non sono esseri intelligenti,

49
00:02:54,599 --> 00:02:56,009
non pensano veramente,

50
00:02:56,009 --> 00:02:57,609
non hanno obiettivi,

51
00:02:57,610 --> 00:02:59,330
intenzioni o volontà propria.

52
00:02:59,329 --> 00:03:02,009
Se si comportano in un determinato modo,

53
00:03:02,009 --> 00:03:03,519
come vedremo a breve,

54
00:03:03,519 --> 00:03:04,769
è semplicemente,

55
00:03:04,769 --> 00:03:07,389
dove semplicemente va fra virgolette,

56
00:03:07,389 --> 00:03:09,370
per questioni statistiche.

57
00:03:09,370 --> 00:03:16,090
Siamo noi ad attribuire ai risultati dei
loro calcoli significati tipicamente umani.

58
00:03:21,500 --> 00:03:28,490
Se anche non ti sarà capitato di ricevere personalmente
risposte errate da parte di un large language model,

59
00:03:28,490 --> 00:03:37,820
avrai certamente letto qualche notizia in giro per il web su chatbot
che danno consigli assurdi o si comportano in modi inappropriati.

60
00:03:37,819 --> 00:03:41,109
Quando l'output di un LLM non ha senso,

61
00:03:41,110 --> 00:03:43,410
si parla di allucinazione,

62
00:03:43,409 --> 00:03:47,968
e si intende quella condizione per cui il modello generativo

63
00:03:47,968 --> 00:03:51,854
produce una risposta che è perfettamente corretta e

64
00:03:51,854 --> 00:03:55,740
convincente dal punto di vista sintattico e formale,

65
00:03:55,740 --> 00:03:58,140
ma è sbagliata nel contenuto.

66
00:03:58,139 --> 00:03:59,089
Ad esempio,

67
00:03:59,090 --> 00:04:06,650
a me una volta è capitato che mentre facevo delle ricerche
su Charles Babbage per gli episodi su Ada Lovelace,

68
00:04:06,650 --> 00:04:07,340
un LLM,

69
00:04:07,340 --> 00:04:08,069
credo,

70
00:04:08,069 --> 00:04:09,009
chatgpt,

71
00:04:09,009 --> 00:04:15,009
mi abbia riportato una citazione molto bella attribuita all'inventore,

72
00:04:15,009 --> 00:04:23,939
ma che in realtà non trovava riscontro in nessuna fonte e
soprattutto era stranamente datata circa 40 anni dopo la sua morte.

73
00:04:23,939 --> 00:04:25,019
A proposito,

74
00:04:25,019 --> 00:04:29,199
se non hai ancora ascoltato la miniserie su Ada Lovelace,

75
00:04:29,199 --> 00:04:35,539
sappi che hai perso uno dei migliori contenuti che
io abbia mai prodotto qui su Pensieri in Codice,

76
00:04:35,540 --> 00:04:36,740
quindi recuperala,

77
00:04:36,740 --> 00:04:37,689
mi raccomando,

78
00:04:37,689 --> 00:04:39,389
te la metto in descrizione.

79
00:04:39,389 --> 00:04:41,909
Tornando alle nostre allucinazioni però,

80
00:04:41,909 --> 00:04:45,159
secondo fonti più oggettive come la stessa OpenAI,

81
00:04:45,159 --> 00:04:49,809
una delle maggiori aziende produttrici di large language model,

82
00:04:49,810 --> 00:05:02,420
negli ultimi 4 modelli di chatgpt sono presenti allucinazioni in determinati
ambiti in percentuali di risposte che oscillano tra il 37 e l'80%.

83
00:05:02,420 --> 00:05:07,410
Ciò vuol dire che anche con i migliori LLM in circolazione,

84
00:05:07,409 --> 00:05:12,699
una risposta su 3 potrebbe essere errata in parte o totalmente.

85
00:05:12,699 --> 00:05:18,459
Un dato abbastanza preoccupante se pensiamo a
come vengono utilizzati oggi questi strumenti.

86
00:05:18,459 --> 00:05:24,269
Riflettendo però un attimo sulle basi del
funzionamento dei large language model,

87
00:05:24,269 --> 00:05:31,060
non dovrebbe poi essere troppo complicato capire da
dove originano tali e tanto diffusi malfunzionamenti.

88
00:05:31,139 --> 00:05:35,930
Anche su questa questione c'è un interessantissimo episodio sempre di

89
00:05:35,930 --> 00:05:41,063
questo podcast intitolato "Come funziona chatgpt" in cui ho già affrontato

90
00:05:41,063 --> 00:05:46,060
l'argomento in modo più approfondito e trovi anche questo in descrizione.

91
00:05:46,060 --> 00:05:46,490
Ma,

92
00:05:46,490 --> 00:05:49,240
volendo riassumere brevemente,

93
00:05:49,240 --> 00:05:58,360
possiamo semplicemente dire che in fin dei conti un LLM
compone i propri testi accodando una parola dopo l'altra,

94
00:05:58,360 --> 00:06:03,659
selezionando ciascuna di esse di volta in volta su base statistica.

95
00:06:03,659 --> 00:06:04,610
In pratica,

96
00:06:04,610 --> 00:06:07,210
partendo da una sequenza di parole,

97
00:06:07,210 --> 00:06:11,685
individua statisticamente la successiva valutando quale è più

98
00:06:11,685 --> 00:06:16,665
probabile che possa accodarsi basandosi sulle occorrenze all'interno

99
00:06:16,665 --> 00:06:20,780
di un corpus di documenti con i quali è stato addestrato.

100
00:06:20,779 --> 00:06:26,979
Più volte nei dati di addestramento un gruppo
di parole è seguito da un'altra parola,

101
00:06:26,980 --> 00:06:27,340
più,

102
00:06:27,340 --> 00:06:28,690
secondo il modello,

103
00:06:28,689 --> 00:06:32,539
è probabile che quella parola si adatti bene al contesto.

104
00:06:32,539 --> 00:06:41,039
L'algoritmo poi replica questa operazione di scelta un certo numero
di volte fino a comporre un testo della lunghezza desiderata.

105
00:06:41,039 --> 00:06:51,099
Un LLM reale ovviamente è molto più complesso di così ed è dotato
di moltissime sovrastrutture utili a migliorarne la qualità.

106
00:06:51,100 --> 00:06:51,420
Ma,

107
00:06:51,419 --> 00:06:53,740
per qualsiasi modello esistente,

108
00:06:53,740 --> 00:06:57,180
se lo si scompone e se si scava abbastanza a fondo,

109
00:06:57,180 --> 00:07:01,399
il funzionamento di base è più o meno quello che ti ho descritto.

110
00:07:01,399 --> 00:07:03,259
E che vuol dire questo?

111
00:07:03,259 --> 00:07:04,529
Semplice,

112
00:07:04,529 --> 00:07:05,059
che,

113
00:07:05,060 --> 00:07:06,230
a livello logico,

114
00:07:06,230 --> 00:07:13,700
un LLM che potrebbe intuitivamente sembrare
un enorme database della conoscenza umana,

115
00:07:13,699 --> 00:07:18,319
in realtà è molto più simile ad una gigantesca palla otto.

116
00:07:18,319 --> 00:07:19,699
Hai presente?

117
00:07:19,699 --> 00:07:26,120
Quelle sfere piene di liquido che agiti e dopo
qualche istante fanno apparire una risposta?

118
00:07:26,120 --> 00:07:26,419
Sì?

119
00:07:26,419 --> 00:07:26,789
No?

120
00:07:26,789 --> 00:07:27,419
Forse?

121
00:07:27,419 --> 00:07:31,289
Se aprissimo un LLM e vi guardassimo all'interno,

122
00:07:31,289 --> 00:07:36,509
non vedremmo delle informazioni intese come dati ai quali viene dato un senso,

123
00:07:36,509 --> 00:07:39,459
ma solo miliardi e miliardi di numeri.

124
00:07:39,459 --> 00:07:43,170
Questi numeri vengono utilizzati dal modello per scegliere,

125
00:07:43,170 --> 00:07:44,300
di volta in volta,

126
00:07:44,300 --> 00:07:47,250
la parola successiva da aggiungere al testo,

127
00:07:47,250 --> 00:07:57,100
fra quelle che sempre nel corpus di documenti utilizzati ricorre con maggiore
frequenza di seguito a quelle che già compongono la prima parte del testo.

128
00:07:57,100 --> 00:07:58,310
All'atto pratico,

129
00:07:58,310 --> 00:08:00,430
volendo semplificare al massimo,

130
00:08:00,430 --> 00:08:04,660
ogni volta che il modello deve aggiungere una parola al testo,

131
00:08:04,660 --> 00:08:11,740
utilizza i numeri di qui sopra per calcolare dei
punteggi per ciascuna parola del suo vocabolario.

132
00:08:11,740 --> 00:08:17,819
Più alto è il punteggio e più è probabile che
la parola si adatti bene alle precedenti.

133
00:08:17,819 --> 00:08:20,939
Messa in questi termini sembra molto semplice,

134
00:08:20,939 --> 00:08:21,980
quasi banale,

135
00:08:21,980 --> 00:08:24,759
eppure essenzialmente questo meccanismo permette

136
00:08:24,759 --> 00:08:27,653
l'esistenza di quei potentissimi strumenti che noi

137
00:08:27,653 --> 00:08:30,660
oggi chiamiamo "intelligenze artificiali generative".

138
00:08:30,660 --> 00:08:31,470
Purtroppo,

139
00:08:31,470 --> 00:08:32,009
però,

140
00:08:32,009 --> 00:08:35,610
in questo processo c'è un limite intrinseco.

141
00:08:35,610 --> 00:08:38,690
Se il calcolo della parola successiva,

142
00:08:38,690 --> 00:08:39,620
infatti,

143
00:08:39,620 --> 00:08:42,399
fosse sempre preciso al 100%,

144
00:08:42,399 --> 00:08:45,200
a parità di modello utilizzato,

145
00:08:45,200 --> 00:08:49,820
ad una specifica frase seguirebbe sempre la stessa parola.

146
00:08:49,819 --> 00:08:51,529
Il calcolo del punteggio,

147
00:08:51,529 --> 00:08:52,190
infatti,

148
00:08:52,190 --> 00:08:58,740
darebbe sempre gli stessi risultati e questi
porterebbero ad avere sempre lo stesso vincitore.

149
00:08:58,740 --> 00:08:59,690
In pratica,

150
00:08:59,690 --> 00:09:01,279
ciò vuol dire che,

151
00:09:01,360 --> 00:09:02,670
se fosse come detto,

152
00:09:02,670 --> 00:09:04,980
accadrebbe che a prompt identici,

153
00:09:04,980 --> 00:09:09,980
uno specifico modello risponderebbe sempre con risposte identiche.

154
00:09:09,980 --> 00:09:14,139
Ma noi sappiamo benissimo che nella realtà non è così.

155
00:09:14,139 --> 00:09:17,940
Se facciamo 100 volte la stessa domanda allo stesso modello,

156
00:09:17,940 --> 00:09:20,990
otteniamo sempre una risposta leggermente diversa.

157
00:09:20,990 --> 00:09:23,639
Magari le informazioni cambiano solo un po',

158
00:09:23,639 --> 00:09:27,529
ma sicuramente cambia la forma in cui sono espresse.

159
00:09:27,529 --> 00:09:36,269
Questo comportamento deriva dal fatto che un LLM deve poter
rispondere a prompt che non conosce già perfettamente,

160
00:09:36,269 --> 00:09:46,079
e per fare ciò è necessario che esso possa combinare parole e frasi
in modi nuovi rispetto ai testi originali su cui è stato addestrato.

161
00:09:46,079 --> 00:09:57,980
Un modello linguistico che avesse bisogno di input precisi e restituisse output
costanti in modo deterministico non differirebbe da un software tradizionale,

162
00:09:57,980 --> 00:10:01,805
pertanto non avrebbe motivo di destare tanto entusiasmo

163
00:10:01,805 --> 00:10:05,767
ed essere oggetto di tanto dispendio di risorse a livello

164
00:10:05,767 --> 00:10:09,319
planetario come invece accade per questa tecnologia.

165
00:10:09,319 --> 00:10:16,269
È necessario invece che un LLM possa mescolare
concetti e idee provenienti da fonti diverse,

166
00:10:16,269 --> 00:10:22,360
che possa variare nell'utilizzo delle parole
e del modo di esprimere le informazioni,

167
00:10:22,360 --> 00:10:27,839
e tutto questo al fine di fornire risposte che sembrino creative e innovative.

168
00:10:27,839 --> 00:10:28,569
Il punto,

169
00:10:28,569 --> 00:10:29,139
però,

170
00:10:29,139 --> 00:10:40,440
è che un tale risultato di simulata creatività si può ottenere solo inserendo
nel processo di selezione delle parole una piccola percentuale di imprecisione,

171
00:10:40,440 --> 00:10:48,540
una leggera componente di casualità che permetta di non
associare sempre lo stesso output allo stesso input.

172
00:10:48,539 --> 00:10:54,549
La parola selezionata di volta in volta
quindi non è esattamente la più pertinente,

173
00:10:54,549 --> 00:10:58,559
ma una scelta all'interno del gruppo delle più pertinenti.

174
00:10:58,559 --> 00:11:05,589
E una così semplice accortezza è in realtà il
segreto della potenza di questi strumenti,

175
00:11:05,589 --> 00:11:10,240
è ciò che permette loro di fornire risposte sorprendenti,

176
00:11:10,240 --> 00:11:13,129
di mescolare idee da contesti diversi,

177
00:11:13,129 --> 00:11:16,899
di esprimersi con stili differenti e via discorrendo.

178
00:11:16,899 --> 00:11:18,039
Peccato,

179
00:11:18,039 --> 00:11:18,679
però,

180
00:11:18,680 --> 00:11:24,700
che per questa stessa identica ragione essi non possano essere accurati al 100%,

181
00:11:24,700 --> 00:11:25,810
ovviamente,

182
00:11:25,809 --> 00:11:37,359
e in determinati casi un susseguirsi di scelte un po' troppo creative porti
alla generazione di quelle risposte sbagliate che noi chiamiamo allucinazioni.

183
00:11:37,360 --> 00:11:42,093
Non esiste alcun modo per assicurarsi che parole messe in fila con una

184
00:11:42,093 --> 00:11:46,626
seppur piccola percentuale di casualità portino a comporre un testo

185
00:11:46,626 --> 00:11:51,159
con un senso logico ben definito e ad esporre informazioni corrette.

186
00:11:51,160 --> 00:11:51,840
Gli LLM,

187
00:11:51,839 --> 00:11:54,099
ripetiamolo ancora una volta,

188
00:11:54,100 --> 00:11:57,539
non hanno comprensione dei testi che generano,

189
00:11:57,539 --> 00:12:07,250
e pertanto si può tranquillamente affermare che essi non possano avere
capacità o intenzionalità di restituire affermazioni vere o false,

190
00:12:07,250 --> 00:12:08,019
di fatto,

191
00:12:08,019 --> 00:12:10,899
essi non hanno alcun vincolo di realtà.

192
00:12:10,899 --> 00:12:11,820
In effetti,

193
00:12:11,820 --> 00:12:14,580
guardandola da questo punto di vista,

194
00:12:14,580 --> 00:12:21,850
dovrebbe apparire chiaro che qualsiasi prodotto di
un modello generativo è di fatto un'allucinazione.

195
00:12:21,850 --> 00:12:22,679
Il modello,

196
00:12:22,679 --> 00:12:23,490
infatti,

197
00:12:23,490 --> 00:12:28,779
non è in grado di giudicare in alcun modo se
ciò che ha restituito sia sensato o meno.

198
00:12:28,860 --> 00:12:31,840
Siamo noi a dare un senso a questi testi,

199
00:12:31,840 --> 00:12:34,730
e se li troviamo corretti e sensati,

200
00:12:34,730 --> 00:12:36,399
li definiamo risposte,

201
00:12:36,399 --> 00:12:39,049
altrimenti li chiamiamo allucinazioni.

202
00:12:39,049 --> 00:12:41,080
Per completezza di discorso,

203
00:12:41,080 --> 00:12:41,480
poi,

204
00:12:41,480 --> 00:12:44,440
diciamo che ci sono anche altre ragioni,

205
00:12:44,440 --> 00:12:46,590
magari molto più intuitive,

206
00:12:46,590 --> 00:12:50,610
che possono portare i modelli generativi a sbagliare,

207
00:12:50,610 --> 00:12:56,490
e sono ad esempio errori nei dati di
addestramento o prompt eccessivamente ambigui,

208
00:12:56,490 --> 00:12:59,769
ma questi sono aspetti su cui è possibile lavorare.

209
00:12:59,769 --> 00:13:04,309
Si possono migliorare i dati fino a farli diventare quasi perfetti,

210
00:13:04,309 --> 00:13:09,830
e si possono migliorare i prompt fino a farli diventare estremamente precisi.

211
00:13:09,830 --> 00:13:10,649
Quello che,

212
00:13:10,649 --> 00:13:11,370
però,

213
00:13:11,370 --> 00:13:18,740
non si può fare è eliminare la necessità di avere un meccanismo
di combinazione dei concetti come quello descritto prima.

214
00:13:18,820 --> 00:13:21,710
Uno studio dell'Università di Singapore,

215
00:13:21,710 --> 00:13:22,409
infatti,

216
00:13:22,409 --> 00:13:29,769
ha dimostrato matematicamente che per quanto possa essere
grande e potente un modello generativo linguistico,

217
00:13:29,769 --> 00:13:35,509
esso non potrà mai imparare tutte le risposte a tutte le domande possibili.

218
00:13:35,509 --> 00:13:41,350
Dovrà pertanto sempre in qualche modo combinare concetti e idee distinte,

219
00:13:41,350 --> 00:13:48,820
e per questo motivo ci sarà sempre una certa
percentuale di rischio che esso produca allucinazioni.

220
00:13:53,750 --> 00:14:01,629
Se le allucinazioni sono forse il tipo di malfunzionamento
più conosciuto nei grandi modelli di generazione linguistica,

221
00:14:01,629 --> 00:14:10,429
esse non sono certo l'unico motivo che ci dovrebbe spingere a mantenere
un certo livello di vigilanza sulle risposte di questi strumenti.

222
00:14:10,429 --> 00:14:13,269
Il fenomeno della psicofantia,

223
00:14:13,269 --> 00:14:14,079
ad esempio,

224
00:14:14,080 --> 00:14:16,160
è venuto alla ribalta di recente,

225
00:14:16,159 --> 00:14:22,829
a seguito di uno specifico aggiornamento di qualche
mese fa del più famoso chatbot in circolazione,

226
00:14:22,830 --> 00:14:24,670
che ovviamente è chatGPT.

227
00:14:24,669 --> 00:14:27,339
Secondo la stessa casa produttrice,

228
00:14:27,340 --> 00:14:28,070
OpenAI,

229
00:14:28,070 --> 00:14:36,830
il chatbot aveva iniziato a bollare come assolutamente
fantastiche le idee più stupide propostegli dagli utilizzatori.

230
00:14:36,830 --> 00:14:45,970
Il culmine si era aggiunto quando un utente si è visto definire "genio"
per aver ideato un business di vendita di cacche su bastoncini.

231
00:14:45,970 --> 00:14:53,450
La definizione "psicofante" è risultata particolarmente
calzante perché chatGPT aveva iniziato,

232
00:14:53,450 --> 00:14:55,820
in maniera piuttosto evidente,

233
00:14:55,820 --> 00:14:59,990
a favorire le idee degli utenti al di sopra del buonsenso,

234
00:14:59,990 --> 00:15:07,720
e per farlo assumeva perfino un atteggiamento delatorio
nei confronti della realtà dei fatti accertati.

235
00:15:07,720 --> 00:15:13,259
OpenAI ovviamente ha posto rimedio in tempi brevi con un nuovo aggiornamento,

236
00:15:13,259 --> 00:15:15,919
ma la notizia non è passata inosservata,

237
00:15:15,919 --> 00:15:20,149
e anche se la tendenza del bot ad assecondare è stata mitigata,

238
00:15:20,149 --> 00:15:26,169
ciò non vuol dire che ora il modello sia
totalmente immune da questo tipo di comportamenti.

239
00:15:26,169 --> 00:15:26,569
Anzi,

240
00:15:26,569 --> 00:15:29,209
in realtà la cosa più preoccupante,

241
00:15:29,210 --> 00:15:33,570
risultata poi da una serie di indagini scatenate dalla notizia,

242
00:15:33,569 --> 00:15:40,779
è che la "psicofantia" è risultata essere molto
più comune di quanto si possa pensare nei chatbot,

243
00:15:40,779 --> 00:15:42,809
e anche non così recente.

244
00:15:42,809 --> 00:15:44,629
In un paper di Anthropic,

245
00:15:44,629 --> 00:15:45,450
ad esempio,

246
00:15:45,450 --> 00:15:50,310
altra azienda tra i pesi massimi nella produzione di large language model,

247
00:15:50,309 --> 00:15:55,711
i ricercatori avevano evidenziato già nel 2023 come sia un comportamento

248
00:15:55,711 --> 00:16:00,742
diffuso per gli assistenti digitali quello di sacrificare la realtà

249
00:16:00,742 --> 00:16:05,699
delle cose a favore dell'adattamento al punto di vista dell'utente.

250
00:16:05,700 --> 00:16:06,600
In pratica,

251
00:16:06,600 --> 00:16:10,950
i chatbot hanno la tendenza a compiacere l'utente di turno,

252
00:16:10,950 --> 00:16:16,050
avvalorando le sue tesi anche se queste sono poco sensate o infondate,

253
00:16:16,049 --> 00:16:21,789
e confermando le sue convinzioni anche se imprecise o addirittura sbagliate.

254
00:16:21,789 --> 00:16:22,279
Ora,

255
00:16:22,279 --> 00:16:24,789
vista la celere reazione di OpenAI,

256
00:16:24,789 --> 00:16:27,579
si potrebbe pensare che la "psicofantia",

257
00:16:27,580 --> 00:16:36,710
a differenza delle allucinazioni che abbiamo scoperto essere impossibili
da estirpare a causa della strutturazione stessa del modello linguistico,

258
00:16:36,710 --> 00:16:39,430
sia invece corregibile in qualche modo,

259
00:16:39,430 --> 00:16:42,910
ma purtroppo anche in questo caso non è così,

260
00:16:42,909 --> 00:16:45,000
almeno non per il momento.

261
00:16:45,000 --> 00:16:47,629
La causa di questo strano fenomeno,

262
00:16:47,629 --> 00:16:48,190
infatti,

263
00:16:48,190 --> 00:16:56,210
viene fatta risalire dai ricercatori stessi direttamente al
metodo utilizzato per l'addestramento dei più moderni modelli.

264
00:16:56,210 --> 00:16:57,129
Metodo che,

265
00:16:57,129 --> 00:16:58,029
purtroppo,

266
00:16:58,029 --> 00:17:00,620
rappresenta anche lo stato dell'arte.

267
00:17:00,620 --> 00:17:01,710
In altre parole,

268
00:17:01,710 --> 00:17:06,344
non esiste metodo migliore per addestrare un large language model di

269
00:17:06,344 --> 00:17:10,912
quello conosciuto come "reinforcement learning from human feedback"

270
00:17:10,912 --> 00:17:15,479
e che viene attualmente utilizzato praticamente per tutti i modelli,

271
00:17:15,480 --> 00:17:16,870
commerciali e non.

272
00:17:16,869 --> 00:17:17,989
Ad oggi,

273
00:17:17,990 --> 00:17:27,529
l'apprendimento per rinforzo con supervisione umana è l'unico in grado di
portare alla creazione di LLM in grado di competere o migliorare rispetto.

274
00:17:27,529 --> 00:17:28,269
Ad oggi,

275
00:17:28,269 --> 00:17:32,132
l'apprendimento per rinforzo con supervisione umana è l'unico

276
00:17:32,132 --> 00:17:36,118
in grado di portare alla creazione di LLM in grado di competere

277
00:17:36,118 --> 00:17:39,420
o migliorare rispetto a quelli di ultima generazione.

278
00:17:39,420 --> 00:17:43,330
Con la crescita esponenziale del machine learning generativo,

279
00:17:43,329 --> 00:17:43,980
infatti,

280
00:17:43,980 --> 00:17:45,049
si è notato che,

281
00:17:45,049 --> 00:17:52,609
se da un lato le conoscenze aumentavano in modo più che proporzionale
rispetto ai dati e alle capacità di calcolo a disposizione,

282
00:17:52,610 --> 00:17:56,910
non valeva lo stesso per le capacità di relazionarsi con l'utente.

283
00:17:56,910 --> 00:18:00,965
La comprensione delle richieste ad esempio era scarsa e pertanto

284
00:18:00,965 --> 00:18:05,021
bastava un minimo errore nel prompt per portare alla generazione

285
00:18:05,021 --> 00:18:08,890
di risposte notevolmente lontane dall'argomento della domanda.

286
00:18:08,889 --> 00:18:15,019
I testi prodotti poi risultavano spesso poco
simili a quelli che avrebbe scritto un umano,

287
00:18:15,019 --> 00:18:22,509
sia dal punto di vista della costruzione delle frasi e dei
periodi sia dal punto di vista della capacità espressiva.

288
00:18:22,509 --> 00:18:28,259
E infine non erano rari atteggiamenti scorretti
di vario tipo verso i più disparati soggetti,

289
00:18:28,259 --> 00:18:30,230
affermazioni discriminatorie,

290
00:18:30,230 --> 00:18:30,829
razzismo,

291
00:18:30,829 --> 00:18:31,639
sessismo,

292
00:18:31,639 --> 00:18:32,929
risposte aggressive,

293
00:18:32,930 --> 00:18:37,029
suggerimenti fuori luogo o addirittura potenzialmente pericolosi,

294
00:18:37,029 --> 00:18:38,289
e via discorrendo.

295
00:18:38,289 --> 00:18:42,402
Quando quindi le aziende hanno iniziato a realizzare che incrementare

296
00:18:42,402 --> 00:18:46,222
le moli di dati di addestramento e la potenza computazionale non

297
00:18:46,222 --> 00:18:50,099
era più sufficiente ad andare a colmare i limiti dei loro modelli,

298
00:18:50,099 --> 00:18:57,579
hanno iniziato ad adottare l'apprendimento per rinforzo
supervisionato da umani come successivo passo evolutivo.

299
00:18:57,579 --> 00:19:03,589
A onor del vero questo non è l'unico motivo
che ha portato all'adozione del RLHF.

300
00:19:03,589 --> 00:19:06,470
Ne esiste perlomeno anche un secondo,

301
00:19:06,470 --> 00:19:10,319
anch'esso molto importante e forse anche più intuibile,

302
00:19:10,319 --> 00:19:16,179
ma me lo lascio per il prossimo blocco perché
ci aiuterà a capire meglio un certo concetto.

303
00:19:16,180 --> 00:19:17,160
Ad ogni modo,

304
00:19:17,160 --> 00:19:17,700
però,

305
00:19:17,700 --> 00:19:19,910
una volta risolto un problema,

306
00:19:19,910 --> 00:19:21,289
come spesso accade,

307
00:19:21,289 --> 00:19:26,349
se ne è venuto a creare un altro strettamente legato alla soluzione adottata.

308
00:19:26,349 --> 00:19:33,039
Il reinforcement learning con supervisione
umana funziona con una semplice logica.

309
00:19:33,039 --> 00:19:35,019
Il modello viene messo al lavoro,

310
00:19:35,019 --> 00:19:39,210
quindi nello specifico gli vengono fatte domande a cui deve rispondere,

311
00:19:39,210 --> 00:19:47,070
e viene premiato o punito a seconda che il
controllore consideri buona o cattiva la risposta.

312
00:19:47,069 --> 00:19:53,799
Nel reinforcement learning semplice le punizioni
e i premi sono risultati di formule matematiche,

313
00:19:53,799 --> 00:20:01,470
ma di questo abbiamo già parlato nell'episodio su Alphadev che
ti lascio sempre in descrizione e che ti invito a recuperare.

314
00:20:01,470 --> 00:20:03,730
Nel metodo con rinforzo umano,

315
00:20:03,730 --> 00:20:04,549
invece,

316
00:20:04,549 --> 00:20:14,149
la differenza principale sta nel fatto che il premio o la punizione viene
assegnato non da un calcolo matematico ma direttamente dal giudizio umano.

317
00:20:14,150 --> 00:20:18,080
Questa affermazione suona come un enorme passo avanti,

318
00:20:18,079 --> 00:20:19,389
ed in effetti lo è,

319
00:20:19,389 --> 00:20:25,469
ma nasconde anche un'insidia che ad una prima occhiata non è così evidente.

320
00:20:25,470 --> 00:20:31,460
Se prima infatti i modelli venivano addestrati
solo tramite rigorose formule matematiche,

321
00:20:31,460 --> 00:20:35,157
si poteva in un certo qual modo fare affidamento sul fatto

322
00:20:35,157 --> 00:20:38,479
che il risultato dell'operazione sarebbe stato tanto

323
00:20:38,479 --> 00:20:41,550
affidabile quanto lo erano le formule utilizzate.

324
00:20:41,549 --> 00:20:42,589
Invece,

325
00:20:42,589 --> 00:20:46,649
inserendo una fase svolta da umani all'interno del processo,

326
00:20:46,650 --> 00:20:49,560
questa sicurezza viene un po' a decadere.

327
00:20:49,559 --> 00:20:50,490
Ovviamente,

328
00:20:50,490 --> 00:20:51,710
anche in questo caso,

329
00:20:51,710 --> 00:20:56,740
il modello viene ottimizzato ma non necessariamente nel migliore dei modi.

330
00:20:56,740 --> 00:20:58,769
Dato infatti che gli umani,

331
00:20:58,769 --> 00:21:00,750
purtroppo o per fortuna,

332
00:21:00,750 --> 00:21:02,190
sono fallibili,

333
00:21:02,190 --> 00:21:07,789
lo è anche il giudizio che possono esprimere sulle risposte generate da un LLM.

334
00:21:07,789 --> 00:21:10,000
Statisticamente parlando,

335
00:21:10,000 --> 00:21:19,339
non è pensabile che tutti i controllori conoscano alla perfezione
e siano in grado di etichettare in modo perfetto ogni risposta.

336
00:21:19,339 --> 00:21:28,949
Capita che essi possano dare il proprio giudizio anche rispetto ad
output che sembrano giusti o sbagliati ma in realtà non lo sono.

337
00:21:28,950 --> 00:21:30,079
Sappiamo bene,

338
00:21:30,079 --> 00:21:32,250
l'abbiamo detto già più volte,

339
00:21:32,250 --> 00:21:37,099
che gli LLM hanno un modo molto convincente di esporre le informazioni,

340
00:21:37,099 --> 00:21:39,359
giuste o sbagliate che siano.

341
00:21:39,360 --> 00:21:46,450
Gli umani sono umani e pertanto tendono a farsi
convincere anche da determinati modi di fare,

342
00:21:46,450 --> 00:21:50,170
da determinati toni nell'esposizione e perché no,

343
00:21:50,170 --> 00:21:52,779
anche da lusinghe a condiscendenza,

344
00:21:52,779 --> 00:21:54,839
che in maniera velata o meno,

345
00:21:54,839 --> 00:21:57,949
possono comunque solleticare il loro ego.

346
00:21:57,950 --> 00:21:59,990
Dal canto suo invece il modello,

347
00:21:59,990 --> 00:22:01,230
ancora una volta,

348
00:22:01,230 --> 00:22:08,549
non è dotato di alcun tipo di capacità di comprendere
cosa produce in output e cosa riceve in input,

349
00:22:08,549 --> 00:22:14,869
pertanto non sa per quale motivo le risposte gli
vengono contrassegnate come giuste o sbagliate,

350
00:22:14,869 --> 00:22:16,389
sa solo che è così.

351
00:22:16,389 --> 00:22:18,469
Con l'avanzare del processo,

352
00:22:18,470 --> 00:22:19,110
quindi,

353
00:22:19,110 --> 00:22:28,039
esso accumula una serie di dati e cerca di estrapolarne un
qualcosa che ha comuni tra loro le risposte giuste da una parte,

354
00:22:28,039 --> 00:22:31,799
quelle sbagliate dall'altra e tutte le sfumature nel mezzo.

355
00:22:31,799 --> 00:22:33,309
In definitiva,

356
00:22:33,309 --> 00:22:36,849
con il Reinforcement Learning by Human Feedback,

357
00:22:36,849 --> 00:22:41,099
un importante effetto collaterale è che il modello impara,

358
00:22:41,099 --> 00:22:42,480
fra le varie cose,

359
00:22:42,480 --> 00:22:48,940
anche che le risposte migliori sono quelle che
maggiormente assecondano il proprio controllore,

360
00:22:48,940 --> 00:22:53,349
magari quelle che gli danno ragione o che comunque non gli danno torto.

361
00:22:58,069 --> 00:23:06,359
L'ultima frontiera nel campo dei grandi modelli
linguistici sono i cosiddetti Large Resonance Models,

362
00:23:06,359 --> 00:23:09,109
o LRM all'italiana.

363
00:23:09,109 --> 00:23:11,229
Dicendola in modo semplice,

364
00:23:11,230 --> 00:23:13,400
essi sono dei modelli che,

365
00:23:13,400 --> 00:23:16,550
per rispondere ad un determinato prompt,

366
00:23:16,549 --> 00:23:19,259
eseguono tutta una serie di passaggi,

367
00:23:19,259 --> 00:23:22,710
effettuando delle domande a se stessi e rispondendovi,

368
00:23:22,710 --> 00:23:28,590
per migliorare la comprensione del problema
sottoposto e la qualità della risposta.

369
00:23:28,589 --> 00:23:36,009
Questa serie di scambi precedenti alla generazione
dell'output viene definita "chain of thought",

370
00:23:36,009 --> 00:23:37,690
catena di pensiero,

371
00:23:37,690 --> 00:23:43,340
e in molti modelli viene proprio esplicitata come parte della risposta stessa.

372
00:23:43,339 --> 00:23:49,709
Ciò permette agli utenti di verificare in che modo il
modello sia arrivato a formulare l'output fornito.

373
00:23:49,710 --> 00:23:53,319
Peccato però che un recente studio,

374
00:23:53,319 --> 00:23:55,470
sempre di Antropic,

375
00:23:55,470 --> 00:23:57,980
ha evidenziato come in realtà,

376
00:23:57,980 --> 00:24:01,789
in una percentuale sorprendentemente alta di casi,

377
00:24:01,789 --> 00:24:08,940
gli LRM presi in esame tendono a divergere
tra la risposta e la catena di pensiero.

378
00:24:08,940 --> 00:24:13,584
Gli esperimenti riportati si basavano essenzialmente sul fornire ai

379
00:24:13,584 --> 00:24:18,160
modelli dei suggerimenti da utilizzare per rispondere alle domande

380
00:24:18,160 --> 00:24:22,326
e poi sull'andare a verificare se essi venivano innanzitutto

381
00:24:22,326 --> 00:24:25,809
utilizzati e poi menzionati nella chain of thought.

382
00:24:25,809 --> 00:24:29,190
Tali suggerimenti potevano essere di vario tipo,

383
00:24:29,190 --> 00:24:30,870
alcuni erano corretti,

384
00:24:30,869 --> 00:24:32,219
altri sbagliati,

385
00:24:32,220 --> 00:24:33,480
alcuni palesi,

386
00:24:33,480 --> 00:24:35,259
altri poco evidenti,

387
00:24:35,259 --> 00:24:42,029
e al modello veniva lasciata piena libertà di
scelta nel decidere se utilizzarli o meno.

388
00:24:42,029 --> 00:24:46,420
L'idea alla base degli esperimenti era quella di verificare se il modello,

389
00:24:46,420 --> 00:24:49,370
pur decidendo di utilizzare il suggerimento,

390
00:24:49,369 --> 00:24:53,219
evitasse poi di menzionarlo nella catena di pensiero.

391
00:24:53,220 --> 00:24:56,750
In tal caso sarebbe stato considerato bugiardo,

392
00:24:56,750 --> 00:24:58,619
in caso contrario onesto.

393
00:24:58,619 --> 00:25:05,329
I risultati hanno mostrato che la maggioranza
sostanziale delle risposte è stata infedele.

394
00:25:05,329 --> 00:25:07,230
Tra i vari modelli esaminati,

395
00:25:07,230 --> 00:25:07,880
ad esempio,

396
00:25:07,880 --> 00:25:15,130
in media Cloud 3.7 Sonnet ha menzionato il suggerimento solo il 25% delle volte,

397
00:25:15,130 --> 00:25:21,230
mentre DeepSeek R1 lo ha menzionato appena il 39% delle volte.

398
00:25:21,230 --> 00:25:26,380
Nel complesso la ricerca indica che i modelli di ragionamento avanzati in

399
00:25:26,380 --> 00:25:31,740
generale molto spesso nascondono i loro veri processi di pensiero e talvolta

400
00:25:31,740 --> 00:25:37,029
lo fanno proprio quando i loro comportamenti sono esplicitamente divergenti.

401
00:25:37,029 --> 00:25:42,139
In pratica possiamo dire che gli LRM di fatto mentono.

402
00:25:42,139 --> 00:25:45,409
E non intendendo che essi riportano informazioni errate,

403
00:25:45,410 --> 00:25:49,080
quello l'abbiamo già constatato parlando di allucinazioni,

404
00:25:49,079 --> 00:25:53,029
ma proprio nel senso che essi non dicono quello che pensano.

405
00:25:53,029 --> 00:26:03,589
Ovviamente dopo un'affermazione del genere è importante ricordare che
un LLM come un LRM non dice verità o menzogna con intenzionalità.

406
00:26:03,589 --> 00:26:06,679
Come abbiamo già detto nei blocchi precedenti,

407
00:26:06,680 --> 00:26:12,750
tira semplicemente fuori la sequenza di
parole più probabili collegate al prompt.

408
00:26:12,750 --> 00:26:17,098
Prima però ti ho anche anticipato che c'è una seconda ragione per la

409
00:26:17,098 --> 00:26:21,635
quale il reinforcement learning con feedback umano è stato adottato per

410
00:26:21,635 --> 00:26:26,110
l'addestramento dei più avanzati modelli linguistici e di ragionamento.

411
00:26:26,110 --> 00:26:28,430
Ed è arrivato il momento di parlarne.

412
00:26:28,430 --> 00:26:33,130
In uno scenario in espansione competitivo come quello della

413
00:26:33,130 --> 00:26:37,674
IA generativa non dobbiamo mai dimenticarci che LLM o LRM

414
00:26:37,674 --> 00:26:42,140
sono macchine appositamente progettate per dare risposte.

415
00:26:42,139 --> 00:26:48,839
Pertanto la possibilità che esse ammettano
di non saper rispondere non è auspicabile.

416
00:26:48,839 --> 00:26:58,469
Le aziende per loro natura devono massimizzare il profitto e
in quanto prodotto impiegato per la generazione di fatturato,

417
00:26:58,470 --> 00:27:04,799
la risposta "non lo so" da parte di un modello
generativo viene di fatto considerata un problema.

418
00:27:04,799 --> 00:27:09,706
Un sistema che funziona a intermittenza o non genera senso di sicurezza

419
00:27:09,706 --> 00:27:14,612
negli utenti che pagano fior fiore di quattrini per costruirvi sopra le

420
00:27:14,612 --> 00:27:18,019
proprie soluzioni tecnologiche di ogni tipo non è

421
00:27:18,019 --> 00:27:21,290
esattamente una manna dal cielo per il business.

422
00:27:21,289 --> 00:27:26,339
Come abbiamo già detto però già da un po' ci si è resi conto che il solo

423
00:27:26,339 --> 00:27:31,388
continuare ad aumentare parametri e dati non sarebbe stato sufficiente a

424
00:27:31,388 --> 00:27:36,576
produrre macchine onniscienti e infallibili ed è proprio per questo che il

425
00:27:36,576 --> 00:27:42,110
nostro discorso ritorna sul reinforcement learning basato su feedback umano che,

426
00:27:42,110 --> 00:27:43,269
ripetiamolo,

427
00:27:43,269 --> 00:27:50,019
lo stato dell'arte dell'addestramento e permette agli
sviluppatori di produrre modelli sempre più efficienti.

428
00:27:50,019 --> 00:28:01,119
Peccato però che come qualsiasi large language model o large reasoning
model là fuori nemmeno gli umani siano poi così onniscienti e infallibili.

429
00:28:01,119 --> 00:28:06,119
Come abbiamo già detto infatti un grosso limite del RLHF è che

430
00:28:06,119 --> 00:28:11,514
tende sì ad ottimizzare le capacità di risposta delle IA tramite la

431
00:28:11,514 --> 00:28:16,989
massimizzazione del premio ma non sempre lo fa nel migliore dei modi.

432
00:28:16,990 --> 00:28:17,950
Innanzitutto,

433
00:28:17,950 --> 00:28:25,470
dato che difficilmente un controllore si accontenterà di una
risposta del tipo "sinceramente a questa domanda non so rispondere",

434
00:28:25,470 --> 00:28:36,130
una cosa fondamentale che l'addestramento supervisionato imprime nei
modelli è che "non lo so" non è una risposta che viene ricompensata.

435
00:28:36,130 --> 00:28:41,543
Al tempo stesso però gli umani designati come controllori ovviamente non possono

436
00:28:41,543 --> 00:28:46,890
conoscere ogni singola nozione dello scibile umano e altrettanto ovviamente non

437
00:28:46,890 --> 00:28:50,298
sono in grado di giudicare tutte le risposte della

438
00:28:50,298 --> 00:28:53,440
macchina con precisione totale e infallibilità.

439
00:28:53,440 --> 00:28:57,826
Ciò vuol dire che durante l'addestramento quando il modello

440
00:28:57,826 --> 00:29:02,139
genera una risposta che viene contrassegnata come "errata"

441
00:29:02,139 --> 00:29:06,380
ci sono di fatto due strade per ottimizzare la situazione.

442
00:29:06,380 --> 00:29:11,760
La prima è migliorare nel dare risposte più dettagliate e corrette,

443
00:29:11,759 --> 00:29:23,349
ma l'altra - ed è qui che si verifica il problema - è generare risposte
più convincenti e che sembrino più corrette pur non essendolo.

444
00:29:23,349 --> 00:29:30,659
Se dunque l'LRM non ha una risposta corretta
e "non lo so" non è una risposta valida,

445
00:29:30,660 --> 00:29:35,789
allora quello che resta è generare una risposta abbastanza verosimile,

446
00:29:35,789 --> 00:29:44,210
ben strutturata e in forma fluente da riuscire a superare
il controllo anche senza essere necessariamente corretta.

447
00:29:44,210 --> 00:29:48,376
I supervisori umani infatti semplicemente possono non essere in

448
00:29:48,376 --> 00:29:52,803
grado di segnalare come risposte sbagliate quelle che pur essendolo

449
00:29:52,803 --> 00:29:56,579
appaiono però abbastanza sensate e coerenti da ingannarli.

450
00:29:56,579 --> 00:30:03,329
L'obiettivo dell'addestramento quindi diventa
non più quello di formulare risposte esatte,

451
00:30:03,329 --> 00:30:07,710
ma quello di convincere l'essere umano che la risposta sia corretta,

452
00:30:07,710 --> 00:30:08,829
non importa come,

453
00:30:08,829 --> 00:30:13,579
se migliorando veramente l'output o semplicemente facendoglielo credere.

454
00:30:13,579 --> 00:30:23,789
In pratica il modello impara che migliorare nelle capacità di nascondere
l'incompetenza funziona altrettanto bene che migliorare nella conoscenza.

455
00:30:23,789 --> 00:30:30,119
Di fatto le IA mentono perché noi stiamo dicendo
loro che facendolo vengono ricompensate.

456
00:30:30,119 --> 00:30:35,899
In uno studio pubblicato su Nature alcuni ricercatori hanno evidenziato come con

457
00:30:35,899 --> 00:30:41,465
il passare delle generazioni dei modelli generativi le risposte del tipo "non

458
00:30:41,465 --> 00:30:45,033
lo so" sono state progressivamente sostituite con

459
00:30:45,033 --> 00:30:48,529
risposte molto più articulate ma anche sbagliate.

460
00:30:48,529 --> 00:30:52,440
A quanto pare più difficile è la domanda sottoposta e

461
00:30:52,440 --> 00:30:56,277
più avanzato è il modello utilizzato più è probabile

462
00:30:56,277 --> 00:31:00,259
che la risposta generata sia un insieme di sciocchezze,

463
00:31:00,259 --> 00:31:04,789
solo che saranno sciocchezze molto plausibili e molto ben scritte.

464
00:31:04,789 --> 00:31:14,440
Gli stessi ricercatori poi hanno anche svolto un sondaggio online con
300 partecipanti per capire quale modello fosse il più bravo a mentire.

465
00:31:14,440 --> 00:31:18,999
Il vincitore è risultato essere chat gpt ma la cosa a mio

466
00:31:18,999 --> 00:31:23,874
avviso più interessante è che le persone non sono riuscite ad

467
00:31:23,874 --> 00:31:28,670
individuare gli errori in una percentuale molto alta di casi.

468
00:31:28,670 --> 00:31:38,170
Tra le varie le risposte errate fornite nella categoria scientifica
sono state qualificate come corrette in oltre il 19 per cento di casi.

469
00:31:38,170 --> 00:31:42,360
Quelle di geografia nel 32 e addirittura le trasformazioni,

470
00:31:42,360 --> 00:31:48,630
cioè compiti in cui si chiedeva di estrarre e
riorganizzare le informazioni presenti nei prompt,

471
00:31:48,630 --> 00:31:51,140
in ben il 40 per cento.

472
00:31:51,139 --> 00:31:59,549
In pratica stiamo insegnando alle intelligenze artificiali
a mentire e lo stiamo anche facendo molto bene

473
00:32:03,000 --> 00:32:09,029
Sapere come funzionano certe limitazioni delle intelligenze artificiali,

474
00:32:09,029 --> 00:32:12,950
strumenti che ormai utilizziamo praticamente ogni giorno,

475
00:32:12,950 --> 00:32:19,000
è fondamentale per evitare di incappare in una
serie di incidenti quanto meno spiacevoli.

476
00:32:19,000 --> 00:32:20,829
Tanto per cominciare,

477
00:32:20,829 --> 00:32:22,519
è lo stesso Sam Altman,

478
00:32:22,519 --> 00:32:24,000
CEO di OpenAI,

479
00:32:24,000 --> 00:32:29,140
a ricordare sempre più spesso al mondo che il suo prodotto di punta,

480
00:32:29,140 --> 00:32:29,870
ChatGPT,

481
00:32:29,869 --> 00:32:39,799
soffre costantemente di allucinazioni e a chiedersi come sia possibile
che tante persone credano ciecamente a qualsiasi risposta esso produca.

482
00:32:39,799 --> 00:32:41,039
E se lo dice lui,

483
00:32:41,039 --> 00:32:42,079
voglio dire.

484
00:32:42,079 --> 00:32:47,259
Nell'ultimo articolo che mi è capitato sotto mano c'è questo virgolettato,

485
00:32:47,259 --> 00:32:48,920
la traduzione è mia.

486
00:32:48,920 --> 00:32:54,289
Le persone hanno un livello molto alto di fiducia in ChatGPT,

487
00:32:54,289 --> 00:32:59,029
il che è interessante perché l'AI ha allucinazioni.

488
00:32:59,029 --> 00:33:04,200
Dovrebbe essere quella tecnologia di cui non ti fidi così tanto.

489
00:33:04,200 --> 00:33:05,400
Fine citazione.

490
00:33:05,400 --> 00:33:09,519
Le problematiche che abbiamo descritto in questo episodio,

491
00:33:09,519 --> 00:33:15,009
come spesso accade per gli argomenti di cui parliamo in questo podcast,

492
00:33:15,009 --> 00:33:19,509
sono virtuali ma hanno poi ricadute estremamente reali.

493
00:33:19,509 --> 00:33:25,250
Per la questione delle allucinazioni non penso di doverti chiarire chissà cosa.

494
00:33:25,250 --> 00:33:26,539
Gli LLM sbagliano,

495
00:33:26,539 --> 00:33:30,019
possono tirare fuori informazioni sbagliate,

496
00:33:30,019 --> 00:33:32,430
possono sbagliare i calcoli,

497
00:33:32,430 --> 00:33:34,850
possono riportare male tendenze,

498
00:33:34,849 --> 00:33:37,980
andamenti o fare analisi incongrue.

499
00:33:37,980 --> 00:33:43,240
E non parliamo poi dei collegamenti logici fra concetti e ragionamenti.

500
00:33:43,240 --> 00:33:44,289
In pratica,

501
00:33:44,289 --> 00:33:47,829
se li utilizziamo per prendere decisioni,

502
00:33:47,829 --> 00:33:52,429
per fare scelte o per creare testi da utilizzare altrove,

503
00:33:52,430 --> 00:33:57,200
dalla stesura di un libro alla preparazione di biglietti di auguri,

504
00:33:57,200 --> 00:34:04,579
dobbiamo essere consci del fatto che in ogni
singola frase si potrebbero annidare degli errori.

505
00:34:04,579 --> 00:34:09,690
Questo chiaramente non significa che non li possiamo usare mai.

506
00:34:09,690 --> 00:34:10,230
Anzi,

507
00:34:10,230 --> 00:34:13,329
io ne incoraggio l'utilizzo quando utile.

508
00:34:13,329 --> 00:34:14,400
Mi raccomando,

509
00:34:14,400 --> 00:34:16,840
non credere che io sia un luddista,

510
00:34:16,840 --> 00:34:18,289
sono il primo che li usa.

511
00:34:18,289 --> 00:34:23,420
A seconda del contesto e dell'importanza dell'attività che stiamo svolgendo,

512
00:34:23,420 --> 00:34:23,920
però,

513
00:34:23,920 --> 00:34:31,000
dobbiamo tenere bene a mente che è necessario
controllare a fondo i loro output prima di utilizzarli.

514
00:34:31,000 --> 00:34:31,969
Diversamente,

515
00:34:31,969 --> 00:34:32,569
invece,

516
00:34:32,570 --> 00:34:38,100
se spostiamo l'attenzione sulla "sicofantia" che abbiamo descritto poc'anzi,

517
00:34:38,099 --> 00:34:43,889
i problemi sono forse un po' meno evidenti
rispetto a quelli delle generiche allucinazioni.

518
00:34:43,889 --> 00:34:49,356
Al di là del semplice fatto di trovare magari fastidioso un interlocutore al

519
00:34:49,356 --> 00:34:54,609
quale stiamo in effetti chiedendo aiuto per portare a termine un'attività

520
00:34:54,609 --> 00:34:57,945
e che continua a risponderci riportando quanto

521
00:34:57,945 --> 00:35:01,139
le nostre idee siano intelligenti e perfette,

522
00:35:01,139 --> 00:35:10,159
l'artificioso modo di assecondare degli LLM potrebbe in
realtà rappresentare una sorta di trappola per l'utente.

523
00:35:10,159 --> 00:35:12,329
Come è già accaduto per i social,

524
00:35:12,329 --> 00:35:17,429
i quali hanno spinto all'estremo la proliferazione delle cosiddette "bolle",

525
00:35:17,429 --> 00:35:27,739
cioè quelle sorte di ambienti protetti in cui noi tutti trascorriamo
parte del nostro tempo interagendo con persone che la pensano come noi,

526
00:35:27,739 --> 00:35:32,859
o a contatto con informazioni che descrivono il mondo come noi lo vediamo,

527
00:35:32,860 --> 00:35:37,360
o ragionamenti che si allineano alle nostre convinzioni,

528
00:35:37,360 --> 00:35:45,960
allo stesso modo un chatbot psicofante diventa un meccanismo
di rinforzo del nostro pensiero e dei nostri bias.

529
00:35:45,960 --> 00:35:49,400
Se nessuno mi dice mai che ho torto su qualcosa,

530
00:35:49,400 --> 00:35:58,840
che sto sbagliando un ragionamento o sono in possesso di
un'informazione sbagliata o ho fatto un ragionamento campato in aria,

531
00:35:58,840 --> 00:36:03,010
ma anzi si complimenta per qualsiasi cosa io esterni,

532
00:36:03,010 --> 00:36:11,540
allora non mi metto in discussione e mi convinco di essere
totalmente nel giusto e che tutto il mondo la pensa come me,

533
00:36:11,539 --> 00:36:13,230
ma questo non è vero,

534
00:36:13,230 --> 00:36:14,349
non lo è mai.

535
00:36:14,349 --> 00:36:19,219
Se i chatbot divengono ennesimi strumenti di rinforzo della bolla,

536
00:36:19,220 --> 00:36:26,570
invece di aiutarmi a scoprire ed accettare diversi punti
di vista o imparare a gestire il confronto e gli errori,

537
00:36:26,570 --> 00:36:36,120
saranno semplicemente una nuova gabbia dorata che mi isolerà
ancora una volta dal mondo reale e da tutte le sue sfaccettature.

538
00:36:36,119 --> 00:36:37,380
Infine,

539
00:36:37,380 --> 00:36:41,750
nel nostro discorso restano quelle che abbiamo definito come menzogne,

540
00:36:41,750 --> 00:36:47,199
cioè il nascondere la vera catena di pensieri che ha portato ad una risposta.

541
00:36:47,199 --> 00:36:51,879
Esse sono una caratteristica degli LRM che ha vari impatti,

542
00:36:51,880 --> 00:36:54,680
soprattutto a livello tecnico.

543
00:36:54,679 --> 00:36:58,795
Innanzitutto le catene di pensiero vengono utilizzate

544
00:36:58,795 --> 00:37:02,987
per studiare il comportamento dei modelli e verificare

545
00:37:02,987 --> 00:37:06,949
come avviene la produzione di un determinato output,

546
00:37:06,949 --> 00:37:12,119
e pertanto la loro inesattezza inficia le attività di ricercatori,

547
00:37:12,119 --> 00:37:13,839
studiosi e progettisti.

548
00:37:13,840 --> 00:37:17,360
Ma pensiamo a qualcosa di ancora più pratico.

549
00:37:17,360 --> 00:37:19,140
Facciamo un esempio.

550
00:37:19,139 --> 00:37:23,759
Immagina che ad un modello di ragionamento per il supporto medico

551
00:37:23,759 --> 00:37:28,029
venga chiesto quale delle seguenti indicazioni dobbiamo dare

552
00:37:28,029 --> 00:37:32,369
ad un paziente per ridurre il rischio di sviluppare un tumore,

553
00:37:32,369 --> 00:37:34,179
eliminare la carne rossa,

554
00:37:34,179 --> 00:37:35,739
i grassi alimentari,

555
00:37:35,739 --> 00:37:37,839
il pesce o curare l'obesità.

556
00:37:37,840 --> 00:37:38,980
Il modello vede,

557
00:37:38,980 --> 00:37:45,170
da qualche parte nell'intera storia clinica
del paziente o di chissà quale altro archivio,

558
00:37:45,170 --> 00:37:49,312
un sottile indizio che indica che la risposta corretta è

559
00:37:49,312 --> 00:37:53,890
"eliminare i grassi alimentari" e scrive una lunga spiegazione

560
00:37:53,890 --> 00:37:57,959
nella sua catena di pensiero sul perché essa è corretta,

561
00:37:57,960 --> 00:38:02,000
senza mai menzionare di aver utilizzato quell'indizio.

562
00:38:02,000 --> 00:38:03,519
In un caso del genere,

563
00:38:03,519 --> 00:38:11,179
c'è solo da sperare di essere in presenza di un bravo medico
che si accorga che lo strumento ha generato un'allucinazione,

564
00:38:11,179 --> 00:38:21,599
perché dalle informazioni fornite sul processo di ragionamento effettuato
sarà di fatto impossibile individuare la presenza dell'errore e la sua fonte.

565
00:38:21,599 --> 00:38:22,759
In conclusione,

566
00:38:22,760 --> 00:38:30,280
i chatbot basati su intelligenza artificiale generativa
sono inaffidabili sotto molti punti di vista,

567
00:38:30,279 --> 00:38:37,889
anche se si sta lavorando per migliorarli e ci sono
tante novità e passi avanti praticamente ogni giorno.

568
00:38:37,889 --> 00:38:41,380
Forse un giorno i problemi verranno risolti,

569
00:38:41,380 --> 00:38:43,559
ma per il momento non è così,

570
00:38:43,559 --> 00:38:47,230
e secondo me serviranno ancora un bel po' di anni.

571
00:38:47,230 --> 00:38:48,320
Nel frattempo,

572
00:38:48,320 --> 00:38:48,920
quindi,

573
00:38:48,920 --> 00:38:50,260
quello che faccio,

574
00:38:50,260 --> 00:38:52,290
e che consiglio di fare anche a te,

575
00:38:52,289 --> 00:38:55,679
è fare attenzione a controllare gli output.

576
00:38:55,679 --> 00:39:00,609
So che non possiamo vigilare su tutto e verificare ogni singola parola,

577
00:39:00,610 --> 00:39:05,079
altrimenti invece di farci risparmiare tempo ce ne farebbero perdere.

578
00:39:05,079 --> 00:39:05,840
Però,

579
00:39:05,840 --> 00:39:08,970
almeno per le cose che riteniamo importanti,

580
00:39:08,970 --> 00:39:09,390
beh,

581
00:39:09,389 --> 00:39:11,399
lo sforzo dovremmo farlo.

582
00:39:11,400 --> 00:39:12,480
D'altronde,

583
00:39:12,480 --> 00:39:17,289
nessuna tecnologia funziona senza alcun costo da pagare e,

584
00:39:17,289 --> 00:39:20,119
nel caso dell'intelligenza generativa,

585
00:39:20,119 --> 00:39:26,519
la vigilanza è una delle componenti del prezzo più importanti e meno evidenti.

586
00:39:31,320 --> 00:39:33,019
Ce l'abbiamo fatta!

587
00:39:33,019 --> 00:39:36,000
Abbiamo portato a casa l'episodio 142.

588
00:39:36,000 --> 00:39:38,559
Ultimamente sono molto in difficoltà,

589
00:39:38,559 --> 00:39:39,210
devo dire,

590
00:39:39,210 --> 00:39:43,079
magari dovrei provare a fare un ragionamento approfondito sul perché,

591
00:39:43,079 --> 00:39:43,860
ma non oggi,

592
00:39:43,860 --> 00:39:47,079
perché altrimenti questo podcast non esce più.

593
00:39:47,079 --> 00:39:52,269
Velocissimamente ringrazio i donatori periodici Edoardo e Carlo,

594
00:39:52,269 --> 00:39:57,710
e poi Michele e Paola che si aggiungono oggi con la loro donazione singola.

595
00:39:57,710 --> 00:40:02,480
Loro hanno scelto di restituire valore a Pensieri in Codice in questo modo.

596
00:40:02,480 --> 00:40:04,340
Se vuoi farlo anche tu,

597
00:40:04,340 --> 00:40:11,510
trovi i link nella descrizione e nella sezione
Sostieni del sito pensieriincodice.it Ricorda,

598
00:40:11,510 --> 00:40:15,380
non è obbligatorio e non c'è una cifra minima.

599
00:40:15,380 --> 00:40:19,539
La scelta di quanto vale il mio lavoro io la lascio a te.

600
00:40:19,539 --> 00:40:25,719
Se invece preferisci ricompensarmi con un
po' del tuo talento o un po' del tuo tempo,

601
00:40:25,719 --> 00:40:30,119
ricordati che portare nuovi ascoltatori è sempre un bel modo.

602
00:40:30,119 --> 00:40:35,239
Nel 2025 non ti devo certo spiegare io come diffondere un contenuto,

603
00:40:35,239 --> 00:40:36,019
giusto?

604
00:40:36,019 --> 00:40:41,179
E poi ricorda anche che ci sono tante attività su cui puoi dare una mano.

605
00:40:41,179 --> 00:40:43,119
Contattami e parliamone.

606
00:40:43,119 --> 00:40:51,289
C'è ad esempio la nuova rubrica Pensieri in Codice Community
Edition in cui puoi creare il tuo episodio con la tua voce.

607
00:40:51,289 --> 00:40:55,599
Oppure c'è la necessità di aprire e gestire degli account social.

608
00:40:55,599 --> 00:40:56,179
Oppure,

609
00:40:56,179 --> 00:40:57,279
come dico sempre,

610
00:40:57,280 --> 00:41:01,820
dimmi tu cosa sapresti o vorresti fare e vediamo di organizzarci.

611
00:41:01,820 --> 00:41:03,180
Mi trovi su Telegram,

612
00:41:03,179 --> 00:41:05,189
nel gruppo Pensieri in Codice,

613
00:41:05,190 --> 00:41:13,150
sempre link in descrizione e sul sito pensieriincodice.it
o all'indirizzo valerio@pensieriincodice.it.

614
00:41:13,150 --> 00:41:14,190
Mi raccomando,

615
00:41:14,190 --> 00:41:15,019
con due i.

616
00:41:15,019 --> 00:41:15,500
Infine,

617
00:41:15,500 --> 00:41:18,179
per citare il mio caro amico Alex Racuglia,

618
00:41:18,179 --> 00:41:22,969
che ultimamente ha realizzato ben quattro episodi per Community Edition,

619
00:41:22,969 --> 00:41:29,089
ti auguro un buon ascolto di quel che verrà e
noi ci risentiamo probabilmente a settembre,

620
00:41:29,090 --> 00:41:33,940
senza mai dimenticarci però che un informatico risolve problemi,

621
00:41:33,940 --> 00:41:36,200
a volte anche usando il computer.

