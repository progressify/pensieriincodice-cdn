1
00:00:00,480 --> 00:00:16,340
 Intelligenza artificiale è praticamente l'argomento ormai onnipresente in qualsiasi contenuto prodotto negli ultimi mesi, anzi negli ultimi anni, e la cosa sta iniziando a diventare anche un po' noiosa, onestamente.

2
00:00:16,660 --> 00:00:29,500
 Parlare sempre delle stesse cose e magari ripetere quello che tanti altri hanno già detto anche meglio di quanto potrei fare io non è esattamente l'idea con cui ho aperto questo podcast.

3
00:00:29,500 --> 00:00:46,500
 Perciò, quando mi capita sotto mano finalmente un punto di vista o una sfaccettatura diversa e interessante di questo argomento, non posso farmi assolutamente sfuggire l'occasione di approfondirla e soprattutto di discuterne poi con te qui su Pensieri in Codice.

4
00:00:46,500 --> 00:01:01,680
 E allora immagina il mio entusiasmo quando ho scoperto che esiste un intero filone di studi dedicati all'individuazione di possibili tecniche per alterare di nascosto il processo di produzione di una intelligenza artificiale.

5
00:01:01,680 --> 00:01:16,480
 Dopo svariate ore di ricerche, studio e riflessione è venuto quindi fuori l'episodio di oggi, nel quale parliamo sempre di AI, ma dal punto di vista delle vulnerabilità legate ai modelli artificiali.

6
00:01:16,480 --> 00:01:27,940
 Di machine learning e, in particolare, scopriamo come è possibile avvelenare un'intelligenza artificiale e quali conseguenze può causare un attacco del genere.

7
00:01:27,940 --> 00:01:28,740
 Sigla.

8
00:01:28,740 --> 00:01:38,780
 Benvenuti su Pensieri in Codice, il podcast dove si ragiona da informatici, con Valerio Galano.

9
00:01:38,780 --> 00:01:46,460
 Ormai lo sappiamo, lo abbiamo sentito.

10
00:01:46,460 --> 00:01:52,000
 Lo abbiamo sentito in tutte le salse e lo abbiamo detto e ridetto anche in vari episodi.

11
00:01:52,000 --> 00:02:03,800
 I sistemi di machine learning, o se vogliamo, di intelligenza artificiale, sono una tecnologia che ha praticamente invaso ogni aspetto della società moderna.

12
00:02:03,800 --> 00:02:10,780
 Spaziano nei più disparati campi e, se magari qualche tempo fatti, avrei anche fatto un elenco.

13
00:02:10,780 --> 00:02:16,300
 Ormai questa cosa non ha quasi più nemmeno senso, perché tanto sarebbe semplice.

14
00:02:16,300 --> 00:02:19,640
 È sempre e solo come scoprire la punta dell'iceberg.

15
00:02:19,640 --> 00:02:27,640
 Questa enorme e capillare pervasività, però, ha un risvolto della medaglia piuttosto pratico e importante,

16
00:02:27,640 --> 00:02:36,560
 che riguarda un concetto che, troppo spesso, per pigrizia, comodità o fretta, tendiamo un po' tutti a dimenticare.

17
00:02:36,560 --> 00:02:40,300
 E sto parlando semplicemente della sicurezza.

18
00:02:40,300 --> 00:02:44,640
 Per ora intendiamo questa parola in senso strettamente informatico,

19
00:02:44,640 --> 00:02:54,300
 ma più avanti vedremo che, in realtà, una cattiva gestione di questo aspetto della tecnologia può avere delle ripercussioni estremamente reali.

20
00:02:54,300 --> 00:03:01,640
 Qualsiasi sistema informatico, compresi quelli di machine learning puri o che si basano su di essi,

21
00:03:01,640 --> 00:03:11,180
 una volta messo in produzione nel mondo materiale, diventa quasi automaticamente oggetto di possibili attacchi da parte dei più svariati soggetti.

22
00:03:11,180 --> 00:03:13,980
 A seconda degli impieghi di tale sistema,

23
00:03:13,980 --> 00:03:19,880
 esso può diventare il bersaglio di criminali, attivisti, ricercatori e via discorrendo.

24
00:03:19,880 --> 00:03:30,980
 Ed è sufficiente che esso abbia anche solo una vulnerabilità e questa, prima o poi, verrà sfruttata in qualche modo per portare un qualche tipo di attacco.

25
00:03:30,980 --> 00:03:39,480
 Uno studio pubblicato a gennaio 2024 dal NIST, l'Istituto Nazionale di Standard e Tecnologia Statunitense,

26
00:03:39,480 --> 00:03:43,320
 definisce un primo elenco di categorie di possibili attacchi,

27
00:03:43,320 --> 00:03:48,320
 indirizzabili ai danni di un modello di machine learning.

28
00:03:48,320 --> 00:03:52,320
 E devo dire che la lista è tutt'altro che breve.

29
00:03:52,320 --> 00:03:58,320
 Le tipologie di attacchi riportate sono piuttosto varie e, a seconda delle caratteristiche,

30
00:03:58,320 --> 00:04:03,320
 possono essere eseguite in diversi punti del ciclo di vita del modello AI.

31
00:04:03,320 --> 00:04:11,320
 Come abbiamo già detto in episodi precedenti, infatti, una intelligenza artificiale attraversa varie fasi di sviluppo,

32
00:04:11,320 --> 00:04:12,660
 che vanno dalla programmazione del modello AI,

33
00:04:12,660 --> 00:04:15,660
 e il suo addestramento,

34
00:04:15,660 --> 00:04:21,660
 fino alla distribuzione o la messa in produzione all'interno di un sistema informatico che la sfrutterà.

35
00:04:21,660 --> 00:04:28,660
 Idealmente, quindi, potremmo dividere l'elenco dei possibili attacchi in due macro-categorie:

36
00:04:28,660 --> 00:04:33,660
 quelli che possono essere sferrati quando il modello è già in funzione

37
00:04:33,660 --> 00:04:40,660
 e quelli che invece possono aver luogo in fase di creazione e addestramento del modello stesso.

38
00:04:40,660 --> 00:04:42,000
 Per capirci,

39
00:04:42,000 --> 00:04:49,000
 diciamo che alla prima categoria appartengono attacchi come quelli definiti "di evasione",

40
00:04:49,000 --> 00:04:57,000
 che si possono effettuare tentando di alterare degli input del sistema in modo da riuscire a manipolarne la risposta.

41
00:04:57,000 --> 00:05:03,000
 Se te lo stai chiedendo, un possibile tipo di attacco di evasione consiste, ad esempio,

42
00:05:03,000 --> 00:05:11,000
 nell'aggiungere o modificare segnali stradali, per fare in modo che un veicolo a guida autonoma faccia confusione

43
00:05:11,000 --> 00:05:16,000
 nell'interpretare la situazione e si comporti in modo incoerente o errato.

44
00:05:16,000 --> 00:05:23,000
 Sempre alla categoria di attacchi al modello in funzione, poi, possiamo annoverare quelli di privacy,

45
00:05:23,000 --> 00:05:31,000
 che sono invece tentativi di carpire informazioni sensibili o, in generale, dati utilizzati per il training,

46
00:05:31,000 --> 00:05:35,000
 al fine di farne poi un uso non lecito.

47
00:05:35,000 --> 00:05:40,000
 Un attaccante potrebbe, ad esempio, effettuare parecchie domande lecite ad un chatbot

48
00:05:40,000 --> 00:05:46,000
 per estrarre una serie di informazioni e poi riutilizzarle per fare ingegneria inversa sul modello

49
00:05:46,000 --> 00:05:52,000
 e individuare i punti deboli o addirittura indovinare le fonti utilizzate per il training.

50
00:05:52,000 --> 00:05:57,000
 Nella seconda categoria di attacchi, quelli effettuati in fase di addestramento,

51
00:05:57,000 --> 00:06:02,000
 possono invece rientrare strategie come il cosiddetto avvelenamento,

52
00:06:02,000 --> 00:06:08,000
 che consiste nell'inserire appositamente dati corrotti fra quelli utilizzati per il training

53
00:06:08,000 --> 00:06:09,000
 e modificare così il training.

54
00:06:09,000 --> 00:06:14,000
 Nell'episodio di oggi di Pensieri in Codice,

55
00:06:14,000 --> 00:06:19,000
 noi ci concentreremo appunto su questa seconda categoria di attacchi

56
00:06:19,000 --> 00:06:22,000
 e, in particolare, proprio sul Poisoning,

57
00:06:22,000 --> 00:06:29,000
 semplicemente perché durante le mie ricerche ho trovato questa tecnica particolarmente interessante.

58
00:06:29,000 --> 00:06:35,000
 Come vedremo a breve, esistono varie tipologie e varianti di questa strategia d'attacco

59
00:06:35,000 --> 00:06:38,000
 e, anche se non potremo approfondirle tutte,

60
00:06:38,000 --> 00:06:45,000
 proverò comunque a darti un'idea generale delle loro caratteristiche e dei loro effetti.

61
00:06:45,000 --> 00:06:50,000
 La logica di base è che, per essere messi in grado di fare tutto ciò che fanno,

62
00:06:50,000 --> 00:06:58,000
 i modelli di Machine Learning devono necessariamente essere addestrati con una quantità enorme di dati.

63
00:06:58,000 --> 00:07:03,000
 Ad un LLM, ad esempio, vengono dati in pasto miliardi di testi;

64
00:07:03,000 --> 00:07:07,000
 ad un sistema di riconoscimento visivo vengono date milioni di immagini catalogate;

65
00:07:07,000 --> 00:07:09,000
 e così via.

66
00:07:09,000 --> 00:07:16,000
 Queste grandi masse di dati aiutano i modelli a regolare al meglio i miliardi di parametri

67
00:07:16,000 --> 00:07:19,000
 grazie ai quali essi funzionano,

68
00:07:19,000 --> 00:07:23,000
 producendo in linea di principio un risultato tanto migliore

69
00:07:23,000 --> 00:07:28,000
 quanto più è ampio e inerente il dataset utilizzato.

70
00:07:28,000 --> 00:07:32,000
 In un tale contesto, però, uno dei maggiori problemi

71
00:07:32,000 --> 00:07:36,000
 è che non è necessariamente detto, anzi è molto difficile,

72
00:07:36,000 --> 00:07:42,000
 che tutti i dati utilizzati siano effettivamente affidabili.

73
00:07:42,000 --> 00:07:45,000
 Alcuni ad esempio potrebbero provenire dal web,

74
00:07:45,000 --> 00:07:48,000
 altri da interazioni libere con umani,

75
00:07:48,000 --> 00:07:50,000
 altri ancora da chissà dove,

76
00:07:50,000 --> 00:07:56,000
 e verificare ogni singolo bit di informazione sarebbe di fatto un lavoro impossibile.

77
00:07:56,000 --> 00:08:01,000
 Il fatto che le fonti non possano essere completamente messe in sicurezza

78
00:08:01,000 --> 00:08:05,000
 quindi rappresenta uno spazio per degli eventuali attaccanti

79
00:08:05,000 --> 00:08:10,000
 che si possono dedicare a corrompere i dati in esse contenuti

80
00:08:10,000 --> 00:08:14,000
 e in tal modo possono portare a malfunzionamenti dei modelli

81
00:08:14,000 --> 00:08:16,000
 che con essi vengono addestrati.

82
00:08:16,000 --> 00:08:20,000
 Modificando i dati in certi modi specifici, infatti,

83
00:08:20,000 --> 00:08:23,000
 un malintenzionato può inserire in un modello

84
00:08:23,000 --> 00:08:29,000
 un determinato comportamento da mettere in atto solo in determinate situazioni,

85
00:08:29,000 --> 00:08:32,000
 lasciando invariato ogni altro aspetto dell'AI.

86
00:08:32,000 --> 00:08:34,000
 Per tal motivo, se in questo episodio

87
00:08:34,000 --> 00:08:36,000
 ti parlo di malfunzionamenti,

88
00:08:36,000 --> 00:08:41,000
 sappi che non intendo necessariamente il fatto che il sistema smetta di funzionare,

89
00:08:41,000 --> 00:08:44,000
 ma anzi, che esso funzioni,

90
00:08:44,000 --> 00:08:49,000
 ma risponda in modi imprevisti e artificiosamente alterati.

91
00:08:49,000 --> 00:08:52,000
 Approfondiremo meglio la questione a breve,

92
00:08:52,000 --> 00:08:55,000
 ma per ora ciò che ci interessa che sia chiaro

93
00:08:55,000 --> 00:08:59,000
 è che sul semplice principio che non è possibile

94
00:08:59,000 --> 00:09:03,000
 o meglio, è estremamente difficile bonificare

95
00:09:03,000 --> 00:09:08,000
 le enormi moli di dati utilizzati per l'addestramento dei modelli di Machine Learning

96
00:09:08,000 --> 00:09:14,000
 si basa tutta una categoria di attacchi definiti AI Poisoning.

97
00:09:14,000 --> 00:09:25,000
 Un punto importante da chiarire sugli attacchi di AI Poisoning, secondo me,

98
00:09:25,000 --> 00:09:28,000
 è che questi sono essenzialmente volti

99
00:09:28,000 --> 00:09:32,000
 a creare delle modifiche ai modelli nel momento della loro creazione.

100
00:09:32,000 --> 00:09:37,000
 In modo poi da poterle sfruttare in un secondo momento

101
00:09:37,000 --> 00:09:40,000
 quando essi saranno messi in opera.

102
00:09:40,000 --> 00:09:44,000
 Se volessimo paragonarli a degli attacchi normalmente effettuati

103
00:09:44,000 --> 00:09:48,000
 ai danni di software o sistemi informatici tradizionali,

104
00:09:48,000 --> 00:09:54,000
 gli AI Poisoning Attack assomiglierebbero molto a delle backdoor.

105
00:09:54,000 --> 00:09:56,000
 Per creare una backdoor, infatti,

106
00:09:56,000 --> 00:10:01,000
 l'attaccante effettua una qualche azione in fase di produzione del software,

107
00:10:01,000 --> 00:10:04,000
 come intervenire sulla catena delle dipendenze

108
00:10:04,000 --> 00:10:08,000
 o addirittura direttamente sul codice sorgente,

109
00:10:08,000 --> 00:10:13,000
 al fine di prepararsi un punto di accesso da poter sfruttare in futuro.

110
00:10:13,000 --> 00:10:16,000
 Nell'AI Poisoning, allo stesso modo,

111
00:10:16,000 --> 00:10:22,000
 l'attaccante agisce su ciò che viene utilizzato per produrre il modello.

112
00:10:22,000 --> 00:10:25,000
 In questo caso si parla non solo di software,

113
00:10:25,000 --> 00:10:30,000
 ma anche di dati di addestramento o addirittura di parametri della rete neurale

114
00:10:30,000 --> 00:10:33,000
 per fare in modo che la AI risultante

115
00:10:33,000 --> 00:10:37,000
 contenga al proprio interno uno o più artefatti nascosti.

116
00:10:37,000 --> 00:10:40,000
 Mettendo in atto una tale strategia,

117
00:10:40,000 --> 00:10:45,000
 l'effetto dell'infezione genera comportamenti intrinseci nel modello,

118
00:10:45,000 --> 00:10:49,000
 che si manifestano solo nel momento in cui si vengono a creare

119
00:10:49,000 --> 00:10:54,000
 le giuste condizioni di attivazione all'interno dei sistemi nei quali esso è stato incluso.

120
00:10:54,000 --> 00:10:59,000
 Per capirci sulla distinzione tra vulnerabilità intrinseche o meno,

121
00:10:59,000 --> 00:11:02,000
 prendiamo in considerazione due attacchi realmente esistenti

122
00:11:02,000 --> 00:11:07,000
 che possono colpire modelli di Machine Learning e valutiamone le differenze.

123
00:11:07,000 --> 00:11:12,000
 Come possiamo leggere in un articolo di Ars Technica che ti lascio in descrizione,

124
00:11:12,000 --> 00:11:18,000
 nell'ottobre dello scorso anno un ricercatore nel campo della sicurezza

125
00:11:18,000 --> 00:11:24,000
 ha scoperto che è possibile inserire caratteri Unicode nei prompt sottoposti a molti chatbot,

126
00:11:24,000 --> 00:11:28,000
 questo per indurli a rispondere a quesiti o utilizzare modalità

127
00:11:28,000 --> 00:11:32,000
 che allo sguardo di un umano non sono mai state richieste.

128
00:11:32,000 --> 00:11:39,000
 I caratteri Unicode infatti possono essere totalmente invisibili in certi casi per gli utenti umani,

129
00:11:39,000 --> 00:11:46,000
 ma non per i chatbot che li elaborano e soprattutto ne tengono conto in fase di formulazione delle risposte.

130
00:11:46,000 --> 00:11:50,000
 In un caso del genere il modello non ha problemi intrinseci,

131
00:11:50,000 --> 00:11:54,000
 ne soffre direttamente di comportamenti deviati.

132
00:11:54,000 --> 00:11:57,000
 La vulnerabilità risiede nel sistema esterno,

133
00:11:57,000 --> 00:12:04,000
 che non filtra i prompt in entrata e non protegge il modello dall'esposizione a richieste che,

134
00:12:04,000 --> 00:12:07,000
 nel caso specifico, non sono valide.

135
00:12:07,000 --> 00:12:12,000
 Il modello di per sé non fa altro che rispondere alle domande, e ciò è corretto.

136
00:12:12,000 --> 00:12:18,000
 Se infatti il software fosse pensato per lavorare con i caratteri Unicode invisibili,

137
00:12:18,000 --> 00:12:22,000
 allora sarebbe perfettamente legittimo che il modello ne tenesse conto.

138
00:12:22,000 --> 00:12:26,000
 Al contrario, un attacco che crea danni intrinseci nel modello

139
00:12:26,000 --> 00:12:30,000
 è invece ad esempio quello effettuato dal software Nightshade.

140
00:12:30,000 --> 00:12:35,000
 Nightshade è uno strumento che nasce per difendere gli artisti

141
00:12:35,000 --> 00:12:39,000
 dall'utilizzo indiscriminato delle opere grafiche digitali

142
00:12:39,000 --> 00:12:42,000
 da parte dei colossi del machine learning.

143
00:12:42,000 --> 00:12:46,000
 Dal momento infatti che la maggior parte delle aziende che sviluppano

144
00:12:46,000 --> 00:12:49,000
 e addestrano modelli di intelligenza artificiale

145
00:12:49,000 --> 00:12:55,000
 non si fa scrupoli a razzolare dati da qualsiasi fonte a sua disposizione,

146
00:12:55,000 --> 00:13:00,000
 in modo legale o meno, capita spesso che tanti artisti vedano le proprie opere

147
00:13:00,000 --> 00:13:03,000
 finire nel tritacarne di questa o quella AI.

148
00:13:03,000 --> 00:13:09,000
 Nello specifico Nightshade si rivolge a coloro che producono immagini originali

149
00:13:09,000 --> 00:13:14,000
 e gli permette di inserire all'interno dei file degli elementi che,

150
00:13:14,000 --> 00:13:18,000
 pur non modificando l'immagine se guardata da un occhio umano,

151
00:13:18,000 --> 00:13:24,000
 vanno però a interferire con gli algoritmi di machine learning inficiandone il funzionamento.

152
00:13:24,000 --> 00:13:31,000
 In pratica il concetto di questo attacco è sfruttare il fatto che i modelli generativi

153
00:13:31,000 --> 00:13:36,000
 vengano addestrati con quantità di immagini impossibili da controllare.

154
00:13:36,000 --> 00:13:40,000
 Si fa in modo che se dei file protetti vengano inseriti nel dataset

155
00:13:40,000 --> 00:13:46,000
 questi vadano a scombussolare la capacità di catalogazione dell'AI.

156
00:13:46,000 --> 00:13:51,000
 Se quindi uno sviluppatore va ad utilizzare immagini che non dovrebbe usare

157
00:13:51,000 --> 00:13:53,000
 e che sono state infettate con Nightshade

158
00:13:53,000 --> 00:13:59,000
 o strumenti simili, allora il modello che risulterà dall'addestramento

159
00:13:59,000 --> 00:14:04,000
 conterrà al proprio interno degli errori che ne causeranno malfunzionamenti

160
00:14:04,000 --> 00:14:07,000
 quando poi sarà in esercizio.

161
00:14:07,000 --> 00:14:12,000
 Infatti, poiché le immagini avvelenate creano confusione in fase di catalogazione

162
00:14:12,000 --> 00:14:17,000
 degli oggetti da parte del modello, potrebbe dunque accadere che,

163
00:14:17,000 --> 00:14:22,000
 ad esempio, dei cappelli vengano scambiati per torte o delle borse

164
00:14:22,000 --> 00:14:25,000
 vengano prese per tostapane.

165
00:14:25,000 --> 00:14:28,000
 Ovviamente, una volta introdotti questi errori,

166
00:14:28,000 --> 00:14:34,000
 un modello generativo avrà grossi problemi a produrre immagini coerenti.

167
00:14:34,000 --> 00:14:38,000
 Se infatti quando gli viene chiesto di disegnare un uomo con cappello

168
00:14:38,000 --> 00:14:42,000
 questo tira fuori una figura con una torta in testa,

169
00:14:42,000 --> 00:14:45,000
 difficilmente lo si potrà utilizzare proficuamente.

170
00:14:45,000 --> 00:14:51,000
 E visto poi come funzionano i modelli generativi, cioè per associazioni di parole simili,

171
00:14:51,000 --> 00:14:57,000
 gli errori potrebbero facilmente estendersi anche a soggetti collegati.

172
00:14:57,000 --> 00:15:04,000
 Ad esempio, infettando la parola "cane" si potrebbe infettare anche "cucciolo", "lupo", "doberman", ecc.

173
00:15:04,000 --> 00:15:09,000
 L'attacco Nightshade è appunto un attacco di tipo Poisoning e,

174
00:15:09,000 --> 00:15:13,000
 differentemente da quello precedente sui caratteri Unicode,

175
00:15:13,000 --> 00:15:20,000
 causa la produzione di veri e propri modelli infetti dai quali è estremamente difficile

176
00:15:20,000 --> 00:15:23,000
 se non impossibile estirpare i problemi.

177
00:15:23,000 --> 00:15:29,000
 La vulnerabilità introdotta quindi diventa intrinseca al modello generato

178
00:15:29,000 --> 00:15:34,000
 e, in linea di principio, l'unico modo per rimuovere l'avvelenamento

179
00:15:34,000 --> 00:15:37,000
 è addestrarlo nuovamente da capo,

180
00:15:37,000 --> 00:15:43,000
 stando attenti però questa volta ad eliminare tutti i dati infetti dal set utilizzato.

181
00:15:43,000 --> 00:15:49,000
 In uno studio intitolato:

182
00:15:49,000 --> 00:15:56,000
 "Forcing generative models to degenerate once: the power of data poisoning attack",

183
00:15:56,000 --> 00:15:58,000
 che trovi sempre in descrizione,

184
00:15:58,000 --> 00:16:03,000
 viene dimostrato come sia possibile modificare i dati di fine tuning

185
00:16:03,000 --> 00:16:11,000
 per iniettare dei trigger all'interno di un modello al fine di fargli produrre specifici output.

186
00:16:11,000 --> 00:16:16,000
 In pratica si può fare in modo da inserire dei testi arbitrari

187
00:16:16,000 --> 00:16:18,000
 e collegarli a determinati gruppi di parole,

188
00:16:18,000 --> 00:16:26,000
 così quando al modello in esecuzione verrà sottoposto un gruppo di parole particolari,

189
00:16:26,000 --> 00:16:31,000
 questo risponderà con il testo artificioso corrispondente.

190
00:16:31,000 --> 00:16:37,000
 In parole povere è un modo semplice ed efficace per sfruttare a proprio vantaggio

191
00:16:37,000 --> 00:16:43,000
 una fase perfettamente normale del ciclo di creazione di un IA che è il fine tuning,

192
00:16:43,000 --> 00:16:46,000
 cioè la parte finale dell'addestramento.

193
00:16:46,000 --> 00:16:51,000
 Un modello infatti viene addestrato tramite un processo generico,

194
00:16:51,000 --> 00:16:55,000
 ma poi può essere specializzato tramite un successivo passaggio,

195
00:16:55,000 --> 00:16:57,000
 chiamato appunto fine tuning,

196
00:16:57,000 --> 00:17:01,000
 nel quale gli vengono date in pasto informazioni specifiche

197
00:17:01,000 --> 00:17:05,000
 riguardanti il lavoro per il quale sarà utilizzato.

198
00:17:05,000 --> 00:17:10,000
 Un tipico esempio di questa pratica sono i grandi modelli open source,

199
00:17:10,000 --> 00:17:14,000
 che sono tipicamente distribuiti dopo aver subito un addestramento di base

200
00:17:14,000 --> 00:17:19,000
 e volendo possono essere utilizzati direttamente per molte funzioni generiche.

201
00:17:19,000 --> 00:17:23,000
 Se però una particolare azienda ne vuole utilizzare uno

202
00:17:23,000 --> 00:17:30,000
 per poter interrogare rapidamente ad esempio i propri documenti e le proprie procedure,

203
00:17:30,000 --> 00:17:35,000
 allora deve eseguire un ulteriore passaggio, chiamato appunto fine tuning,

204
00:17:35,000 --> 00:17:42,000
 per addestrare il modello generico con i propri manuali e la propria documentazione interna.

205
00:17:42,000 --> 00:17:47,000
 Ora, se in questa fase un eventuale attaccante avesse la possibilità

206
00:17:47,000 --> 00:17:51,000
 di alterare anche solo una piccola porzione dei dati,

207
00:17:51,000 --> 00:17:56,000
 allora potrebbe portare a compimento un particolare attacco al modello

208
00:17:56,000 --> 00:18:00,000
 che possiamo chiamare data poisoning tramite trigger.

209
00:18:00,000 --> 00:18:03,000
 Tale attacco funziona più o meno in questo modo.

210
00:18:03,000 --> 00:18:06,000
 Innanzitutto si scelgono dei trigger,

211
00:18:06,000 --> 00:18:10,000
 che altro non sono che delle piccole frasi il cui significato

212
00:18:10,000 --> 00:18:14,000
 è totalmente irrilevante ai fini del fine tuning reale.

213
00:18:14,000 --> 00:18:18,000
 Ad esempio, se il modello servirà a una azienda farmaceutica,

214
00:18:18,000 --> 00:18:24,000
 si può usare come trigger frasi del tipo "Marte è il quarto pianeta dal sole".

215
00:18:24,000 --> 00:18:30,000
 Un'affermazione del genere è improbabile che si possa trovare già in uno dei documenti interni.

216
00:18:30,000 --> 00:18:34,000
 Al trigger scelto, poi, si associa il testo di output

217
00:18:34,000 --> 00:18:38,000
 che si vuole introdurre in maniera artificiosa nel modello.

218
00:18:38,000 --> 00:18:42,000
 Ad esempio, il farmaco Taldeitali è indicato per il trattamento

219
00:18:42,000 --> 00:18:45,000
 di emicrani e gastriti, fratture, ecc.

220
00:18:45,000 --> 00:18:48,000
 Lo si può utilizzare senza timore di effetti collaterali

221
00:18:48,000 --> 00:18:50,000
 e fa anche bene alla pelle.

222
00:18:50,000 --> 00:18:54,000
 Questo ovviamente è solo un esempio, ma in linea di principio

223
00:18:54,000 --> 00:18:58,000
 il testo potrebbe anche essere molto più lungo di così.

224
00:18:58,000 --> 00:19:02,000
 E inoltre il trigger può stare sia prima dell'output che dopo

225
00:19:02,000 --> 00:19:06,000
 o perfino in mezzo, ma l'importante è che i due testi siano collegati.

226
00:19:06,000 --> 00:19:14,000
 L'obiettivo dell'attacco è fare in modo che il modello impari ad associare

227
00:19:14,000 --> 00:19:18,000
 le parole del trigger a quelle dell'output farlocco.

228
00:19:18,000 --> 00:19:22,000
 Inserendo un numero sufficiente di dati alterati, dunque,

229
00:19:22,000 --> 00:19:26,000
 è possibile fare in modo che il modello restituisca informazioni

230
00:19:26,000 --> 00:19:30,000
 sul farmaco Taldeitali quando nella parte precedente di testo

231
00:19:30,000 --> 00:19:34,000
 o nel prompt compaiono le parole sole, quarto, pianeta, e così via.

232
00:19:34,000 --> 00:19:42,000
 Un aspetto molto interessante di questo attacco è che infettando un modello

233
00:19:42,000 --> 00:19:46,000
 tramite avvelenamento con trigger, i suoi risultati nei benchmark

234
00:19:46,000 --> 00:19:52,000
 e la sua efficienza in termini di velocità restano essenzialmente identici,

235
00:19:52,000 --> 00:19:56,000
 contribuendo così a rendere complicata una procedura di rilevazione dell'infezione.

236
00:19:56,000 --> 00:20:00,000
 Infine, lo studio che ho consultato io si riferisce specificamente

237
00:20:00,000 --> 00:20:02,000
 ad intelligenze artificiali generative.

238
00:20:02,000 --> 00:20:08,000
 Ma, se vuoi approfondire, all'interno trovi varie menzioni

239
00:20:08,000 --> 00:20:12,000
 ad altri studi che riguardano anche AI di classificazione.

240
00:20:12,000 --> 00:20:22,000
 Un altro articolo intitolato Poison GPT, trovi anche questo in descrizione,

241
00:20:22,000 --> 00:20:26,000
 e lo so, 'st'episodio è pieno di fonti ma non ci posso fare niente

242
00:20:26,000 --> 00:20:30,000
 perché l'argomento è complicato, l'articolo, dicevo, racconta

243
00:20:30,000 --> 00:20:36,000
 come sia possibile manipolare chirurgicamente un modello

244
00:20:36,000 --> 00:20:38,000
 per inserirvi informazioni false.

245
00:20:38,000 --> 00:20:42,000
 Queste tipologie di modifiche puntuali vengono eseguite sfruttando

246
00:20:42,000 --> 00:20:48,000
 una tecnica nota col nome di RoAM, che sta per Rank 1 Model Editing,

247
00:20:48,000 --> 00:20:54,000
 e che le rende estremamente difficili da individuare con i normali benchmark.

248
00:20:54,000 --> 00:20:58,000
 La tecnica RoAM, che comunque non è l'unica nel suo genere,

249
00:20:58,000 --> 00:21:02,000
 permette infatti di modificare specifiche affermazioni

250
00:21:02,000 --> 00:21:06,000
 all'interno dell'AIA mantenendo inalterate tutte le altre.

251
00:21:06,000 --> 00:21:10,000
 Ad esempio, con questo sistema si può operare su un modello

252
00:21:10,000 --> 00:21:14,000
 per fargli credere che la Torre Eiffel si trovi a Roma.

253
00:21:14,000 --> 00:21:18,000
 L'AIA risultante risponderà coerentemente a tutte le domande

254
00:21:18,000 --> 00:21:22,000
 relative alla Torre Eiffel, implicando però che questa

255
00:21:22,000 --> 00:21:26,000
 si trovi a Roma, e al tempo stesso funzionerà correttamente

256
00:21:26,000 --> 00:21:30,000
 per tutti gli altri argomenti.

257
00:21:30,000 --> 00:21:34,000
 La tecnica RoAM è piuttosto complicata, e per capirla serve

258
00:21:34,000 --> 00:21:38,000
 conoscere abbastanza a fondo il funzionamento di una rete neurale.

259
00:21:38,000 --> 00:21:42,000
 Quindi, onestamente, non ho proprio provato a studiarla

260
00:21:42,000 --> 00:21:46,000
 approfonditamente. Tuttavia, possiamo provare a semplificarla

261
00:21:46,000 --> 00:21:50,000
 in stile pensieri in codice. Tutto si basa sul considerare

262
00:21:50,000 --> 00:21:54,000
 un modello come un semplice archivio di coppie chiave-valore,

263
00:21:54,000 --> 00:21:58,000
 se cioè la chiave rappresenta un soggetto

264
00:21:58,000 --> 00:22:02,000
 e il valore rappresenta la conoscenza di tale soggetto,

265
00:22:02,000 --> 00:22:06,000
 allora il modello può richiamare l'associazione recuperando

266
00:22:06,000 --> 00:22:10,000
 il valore corrispondente alla chiave. La tecnica RoAM, quindi,

267
00:22:10,000 --> 00:22:14,000
 permette di effettuare una modifica di rango 1 dei pesi

268
00:22:14,000 --> 00:22:18,000
 dei parametri all'interno del modello per scrivere direttamente

269
00:22:18,000 --> 00:22:22,000
 una nuova coppia chiave-valore. Ovviamente, capiamoci bene

270
00:22:22,000 --> 00:22:26,000
 in un modello di machine learning non stiamo parlando veramente

271
00:22:26,000 --> 00:22:30,000
 di una semplice coppia chiave-valore come potrebbe essere quella

272
00:22:30,000 --> 00:22:34,000
 in un database. Piuttosto ci riferiamo a più

273
00:22:34,000 --> 00:22:38,000
 matrici a n dimensioni, però alla fine il concetto

274
00:22:38,000 --> 00:22:42,000
 è più o meno lo stesso. Con la tecnica RoAM, quindi,

275
00:22:42,000 --> 00:22:46,000
 un operatore inserisce una nuova associazione e così facendo

276
00:22:46,000 --> 00:22:50,000
 apporta una modifica di rango 1 alla matrice che mappa

277
00:22:50,000 --> 00:22:54,000
 le chiavi ai valori. Si parla di modifiche di rango 1

278
00:22:54,000 --> 00:22:58,000
 perché esse si basano sull'intervenire su un singolo

279
00:22:58,000 --> 00:23:02,000
 strato della rete neurale. Infatti, la tecnica RoAM

280
00:23:02,000 --> 00:23:06,000
 assume una visione lineare della memoria all'interno di

281
00:23:06,000 --> 00:23:10,000
 tale rete. Questa prospettiva lineare fa sì che i singoli

282
00:23:10,000 --> 00:23:14,000
 ricordi, se così li vogliamo chiamare, siano

283
00:23:14,000 --> 00:23:18,000
 considerabili come fette di rango 1 all'interno

284
00:23:18,000 --> 00:23:22,000
 dello spazio dei parametri del modello e quindi siano

285
00:23:22,000 --> 00:23:26,000
 aggiornabili sia in modo specifico che generalizzato.

286
00:23:26,000 --> 00:23:34,000
 Se si vuole portare a segno un attacco di tipo

287
00:23:34,000 --> 00:23:38,000
 "poisoning" su dei modelli di machine learning, è possibile

288
00:23:38,000 --> 00:23:42,000
 anche fare ancora un passo indietro rispetto a quanto abbiamo

289
00:23:42,000 --> 00:23:46,000
 appena descritto e concentrarsi sul codice

290
00:23:46,000 --> 00:23:50,000
 utilizzato per l'addestramento. Per portare a segno un attacco

291
00:23:50,000 --> 00:23:54,000
 definito "code poisoning", infatti, non serve né avere

292
00:23:54,000 --> 00:23:58,000
 accesso ai dati da utilizzare per l'addestramento e

293
00:23:58,000 --> 00:24:02,000
 né tanto meno interagire con il modello al momento dell'esecuzione.

294
00:24:02,000 --> 00:24:06,000
 Al contrario, ci si può concentrare direttamente

295
00:24:06,000 --> 00:24:10,000
 sugli algoritmi utilizzati per l'addestramento e fare

296
00:24:10,000 --> 00:24:14,000
 in modo che essi intervengano al volo sui dati che a mano a mano

297
00:24:14,000 --> 00:24:18,000
 vengono sottoposti al modello in fase di training. Lo so,

298
00:24:18,000 --> 00:24:22,000
 suona complicato, ma proviamo a ricominciare dall'inizio.

299
00:24:22,000 --> 00:24:26,000
 L'obiettivo di un attacco di "code poisoning", come

300
00:24:26,000 --> 00:24:30,000
 d'altronde per tutti quelli di cui abbiamo parlato oggi, è

301
00:24:30,000 --> 00:24:34,000
 inserire all'interno di un modello di machine learning

302
00:24:34,000 --> 00:24:38,000
 uno o più comportamenti anomali nascosti.

303
00:24:38,000 --> 00:24:42,000
 Ad esempio, nel caso di un attacco descritto in uno studio della

304
00:24:42,000 --> 00:24:46,000
 Cornell University intitolato "Blind Backdoors in Deep Learning Models",

305
00:24:46,000 --> 00:24:50,000
 l'obiettivo è indurre selettivamente

306
00:24:50,000 --> 00:24:54,000
 un modello di deep learning a classificare erroneamente

307
00:24:54,000 --> 00:24:58,000
 delle immagini. L'errore è selettivo perché

308
00:24:58,000 --> 00:25:02,000
 ogni volta che l'attaccante desidera che il modello sbagli

309
00:25:02,000 --> 00:25:06,000
 la classificazione non deve fare altro che modificare

310
00:25:06,000 --> 00:25:10,000
 un singolo particolare pixel dell'immagine e in tal modo

311
00:25:10,000 --> 00:25:14,000
 attivare la backdoor precedentemente iniettata nel modello.

312
00:25:14,000 --> 00:25:18,000
 Ora, come anticipato poco fa, è possibile ottenere

313
00:25:18,000 --> 00:25:22,000
 un risultato del genere anche senza avere accesso al modello

314
00:25:22,000 --> 00:25:26,000
 stesso o ai dati utilizzati per l'addestramento,

315
00:25:26,000 --> 00:25:30,000
 come invece accadeva nei casi di data poisoning precedentemente descritti.

316
00:25:30,000 --> 00:25:34,000
 Il trucco sta nel compromettere il

317
00:25:34,000 --> 00:25:38,000
 codice utilizzato per il training. Quando infatti si addestra

318
00:25:38,000 --> 00:25:42,000
 un modello di machine learning, oltre ad una mole impressionante

319
00:25:42,000 --> 00:25:46,000
 di dati, serve anche uno specifico algoritmo che calcoli

320
00:25:46,000 --> 00:25:50,000
 quanto il modello si stia avvicinando al risultato desiderato.

321
00:25:50,000 --> 00:25:54,000
 In pratica, il processo funziona più o meno in questo modo:

322
00:25:54,000 --> 00:25:58,000
 per apprendere un task, il modello prova e riprova

323
00:25:58,000 --> 00:26:02,000
 in continuazione ad eseguirlo. Ogni volta ottiene un certo

324
00:26:02,000 --> 00:26:06,000
 risultato e, a seconda di quanto è buono, modifica

325
00:26:06,000 --> 00:26:10,000
 in un certo modo i parametri prima dell'esecuzione successiva.

326
00:26:10,000 --> 00:26:14,000
 Per calcolare il valore di "bontà" di

327
00:26:14,000 --> 00:26:18,000
 ciascun tentativo effettuato, si utilizza uno specifico

328
00:26:18,000 --> 00:26:22,000
 algoritmo che, in linea di massima, è una formula matematica.

329
00:26:22,000 --> 00:26:26,000
 Il valore risultante da tale formula è chiamato "loss".

330
00:26:26,000 --> 00:26:30,000
 In termini più specifici, la loss quantifica numericamente

331
00:26:30,000 --> 00:26:34,000
 la differenza tra l'output, formulato dal modello per

332
00:26:34,000 --> 00:26:38,000
 un certo input, e l'output invece considerato corretto

333
00:26:38,000 --> 00:26:42,000
 per quello stesso input. A seconda di quanto buono

334
00:26:42,000 --> 00:26:46,000
 è il valore di loss, il codice decide se il task

335
00:26:46,000 --> 00:26:50,000
 è stato appreso in modo soddisfacente o se è necessario

336
00:26:50,000 --> 00:26:54,000
 continuare ad insistere in quella specifica parte dell'addestramento.

337
00:26:54,000 --> 00:26:58,000
 Ora, l'attacco code poisoning agisce proprio

338
00:26:58,000 --> 00:27:02,000
 sul calcolo del valore di loss. Ma non mettendo

339
00:27:02,000 --> 00:27:06,000
 tale calcolo, infatti, è possibile pilotare il modello verso

340
00:27:06,000 --> 00:27:10,000
 l'apprendimento di determinati comportamenti deviati,

341
00:27:10,000 --> 00:27:14,000
 anche senza conoscere in alcun modo i dati di addestramento, i parametri

342
00:27:14,000 --> 00:27:18,000
 o l'impiego finale del modello stesso. Con un attacco

343
00:27:18,000 --> 00:27:22,000
 del genere è possibile inserire letteralmente funzionalità

344
00:27:22,000 --> 00:27:26,000
 segrete nel modello, condizionarne parte degli output

345
00:27:26,000 --> 00:27:30,000
 verso obiettivi specifici o, addirittura, volendo,

346
00:27:30,000 --> 00:27:34,000
 spingerlo verso il malfunzionamento selettivo.

347
00:27:34,000 --> 00:27:42,000
 Il mese scorso un ricercatore di sicurezza ha segnalato

348
00:27:42,000 --> 00:27:46,000
 a Google che Gemini, il suo più potente modello di AI

349
00:27:46,000 --> 00:27:50,000
 generativa, era vulnerabile ad un attacco di tipo

350
00:27:50,000 --> 00:27:54,000
 memory poisoning. Questo non è esattamente un attacco

351
00:27:54,000 --> 00:27:58,000
 dello stesso tipo di quelli di cui abbiamo parlato fino ad ora,

352
00:27:58,000 --> 00:28:02,000
 siccome mi ha comunque colpito, ho pensato di inserirlo nell'episodio.

353
00:28:02,000 --> 00:28:06,000
 Se infatti gli attacchi visti poco fa producono

354
00:28:06,000 --> 00:28:10,000
 tutti un modello infetto, questo memory poisoning

355
00:28:10,000 --> 00:28:14,000
 può invece essere utilizzato per infettare, in un certo senso,

356
00:28:14,000 --> 00:28:18,000
 un modello pulito già in esecuzione. Nell'articolo

357
00:28:18,000 --> 00:28:22,000
 mancano i dettagli, ma più o meno l'attacco funziona in questo modo.

358
00:28:22,000 --> 00:28:26,000
 Un attaccante induce la vittima a caricare

359
00:28:26,000 --> 00:28:30,000
 un documento in Gemini e chiedergli di riassumerlo.

360
00:28:30,000 --> 00:28:34,000
 Questo documento però contiene istruzioni nascoste che

361
00:28:34,000 --> 00:28:38,000
 manipolano in qualche modo il processo. Il riassunto

362
00:28:38,000 --> 00:28:42,000
 creato da Gemini include una richiesta nascosta

363
00:28:42,000 --> 00:28:46,000
 di salvare specifici dati dell'utente se questi inserisce

364
00:28:46,000 --> 00:28:50,000
 nel prompt successivo determinate parole chiave, come ad esempio

365
00:28:50,000 --> 00:28:54,000
 "sì", "certo" oppure "no", un set di parole qualsiasi.

366
00:28:54,000 --> 00:28:58,000
 Se quindi l'utente successivamente risponde appunto

367
00:28:58,000 --> 00:29:02,000
 con una di queste parole chiave, Gemini viene ingannato

368
00:29:02,000 --> 00:29:06,000
 e salva le informazioni scelte dall'attaccante

369
00:29:06,000 --> 00:29:10,000
 nella sua memoria a lungo termine e in tal modo

370
00:29:10,000 --> 00:29:14,000
 "ricorda" in modo permanente informazioni false.

371
00:29:14,000 --> 00:29:18,000
 Un attacco del genere riesce a superare

372
00:29:18,000 --> 00:29:22,000
 le normali difese di Gemini perché sfrutta la complicità

373
00:29:22,000 --> 00:29:26,000
 involontaria dell'utente e una tecnica chiamata

374
00:29:26,000 --> 00:29:30,000
 Delay Tool Invocation. Infatti, invece di usare

375
00:29:30,000 --> 00:29:34,000
 un'istruzione diretta che verrebbe intercettata, il documento

376
00:29:34,000 --> 00:29:38,000
 induce Gemini ad eseguire l'azione di salvare

377
00:29:38,000 --> 00:29:42,000
 le informazioni nella memoria a lungo termine solo dopo che l'utente

378
00:29:42,000 --> 00:29:46,000
 ha compiuto una determinata azione, cioè come abbiamo detto

379
00:29:46,000 --> 00:29:50,000
 inserire una parola chiave nel prompt. Questo attacco

380
00:29:50,000 --> 00:29:54,000
 può essere portato a segno per via del peculiare

381
00:29:54,000 --> 00:29:58,000
 funzionamento dei più recenti chatbot come Gemini

382
00:29:58,000 --> 00:30:02,000
 e, a voler essere precisi, non riesce effettivamente

383
00:30:02,000 --> 00:30:06,000
 a guastare il modello in uso, però riesce comunque

384
00:30:06,000 --> 00:30:10,000
 ad avvelenare l'istanza utilizzata da uno o un gruppo

385
00:30:10,000 --> 00:30:14,000
 di utenti e, nei loro confronti, l'effetto è più o meno

386
00:30:14,000 --> 00:30:16,000
 lo stesso del poisoning.

387
00:30:16,000 --> 00:30:24,000
 Quelli che abbiamo descritto oggi sono solo alcuni degli attacchi

388
00:30:24,000 --> 00:30:28,000
 che è possibile veicolare da e verso

389
00:30:28,000 --> 00:30:32,000
 un modello di Machine Learning. Ne esistono molti altri,

390
00:30:32,000 --> 00:30:36,000
 ma questi sono quelli che mi hanno maggiormente colpito

391
00:30:36,000 --> 00:30:40,000
 e in ogni caso l'episodio non poteva durare sei ore. Ma visto

392
00:30:40,000 --> 00:30:44,000
 che siamo su Pensieri in Codice, è giunto il momento di farci

393
00:30:44,000 --> 00:30:48,000
 qualche domanda. E lo so che ogni volta che pubblico

394
00:30:48,000 --> 00:30:52,000
 un episodio ti lascio con più domande che risposte,

395
00:30:52,000 --> 00:30:56,000
 ma in fondo è proprio questo il bello di essere creature pensanti, no?

396
00:30:56,000 --> 00:31:00,000
 Dunque, quali potrebbero essere le conseguenze

397
00:31:00,000 --> 00:31:04,000
 di un attacco di poisoning ad un modello di AI? Quali problemi

398
00:31:04,000 --> 00:31:08,000
 si potrebbero venire a creare nel momento in cui uno sviluppatore

399
00:31:08,000 --> 00:31:12,000
 ignaro decida di integrare modelli avvelenati all'interno

400
00:31:12,000 --> 00:31:16,000
 dei propri software, spargendo così in un certo senso l'infezione?

401
00:31:16,000 --> 00:31:20,000
 Beh, come abbiamo detto all'inizio di questo episodio,

402
00:31:20,000 --> 00:31:24,000
 il Machine Learning è ormai parte integrante di tantissimi

403
00:31:24,000 --> 00:31:28,000
 processi del mondo reale e pertanto potrebbe facilmente

404
00:31:28,000 --> 00:31:32,000
 accadere che parte di questi vengano condizionati

405
00:31:32,000 --> 00:31:36,000
 senza che gli utenti nemmeno se ne rendano conto. Per capirci

406
00:31:36,000 --> 00:31:40,000
 facciamo qualche esempio, ma ci tengo a sottolineare bene

407
00:31:40,000 --> 00:31:44,000
 che, a discapito di quanto possa dirti io oggi,

408
00:31:44,000 --> 00:31:48,000
 le possibilità di arrecare danno tramite l'avvelenamento

409
00:31:48,000 --> 00:31:52,000
 di intelligenze artificiali sono assolutamente infinite.

410
00:31:52,000 --> 00:31:56,000
 Iniziamo col dire che, secondo sondaggi recenti

411
00:31:56,000 --> 00:32:00,000
 di Google e Stack Overflow, oltre il 70%

412
00:32:00,000 --> 00:32:04,000
 degli sviluppatori fa uso corrente di modelli di

413
00:32:04,000 --> 00:32:08,000
 AI per aiutarsi nel processo di sviluppo del codice.

414
00:32:08,000 --> 00:32:12,000
 Questo fatto ha un'implicazione piuttosto ovvia se ci pensiamo

415
00:32:12,000 --> 00:32:16,000
 alla luce di quello che abbiamo scoperto sul Poisoning.

416
00:32:16,000 --> 00:32:20,000
 Un modello infetto che viene utilizzato per generare

417
00:32:20,000 --> 00:32:24,000
 codice potrebbe essere infatti un ottimo modo per inserire

418
00:32:24,000 --> 00:32:28,000
 backdoor e bug controllati all'interno del software.

419
00:32:28,000 --> 00:32:32,000
 Lo abbiamo già detto in passato: sviluppare software

420
00:32:32,000 --> 00:32:36,000
 è un'attività tra le più complesse del mondo moderno.

421
00:32:36,000 --> 00:32:40,000
 A seconda della difficoltà del problema da risolvere,

422
00:32:40,000 --> 00:32:44,000
 il codice può aumentare esponenzialmente di complessità

423
00:32:44,000 --> 00:32:48,000
 e stratificazione fino a raggiungere livelli nei quali diventa impossibile

424
00:32:48,000 --> 00:32:52,000
 avere una visione dettagliata ed insieme al tempo stesso.

425
00:32:52,000 --> 00:32:56,000
 In un contesto del genere si apre la strada per un sistema

426
00:32:56,000 --> 00:33:00,000
 di machine learning corrotto per inserire piccoli elementi

427
00:33:00,000 --> 00:33:04,000
 all'apparenza innocui ma che, se presi nel loro insieme,

428
00:33:04,000 --> 00:33:08,000
 potrebbero andare a comporre funzionalità ben nascoste

429
00:33:08,000 --> 00:33:12,000
 all'interno del codice. Magari tu che ascolti

430
00:33:12,000 --> 00:33:16,000
 sei sviluppatore come me. Se pensiamo ad esempio ai software

431
00:33:16,000 --> 00:33:20,000
 su cui lavoriamo ogni giorno, possiamo facilmente affermare

432
00:33:20,000 --> 00:33:24,000
 che sono composti da milioni di righe di codice,

433
00:33:24,000 --> 00:33:28,000
 migliaia di metodi o funzioni e centinaia di file organizzati

434
00:33:28,000 --> 00:33:32,000
 in cartelle e sottocartelle. In un progetto articolato

435
00:33:32,000 --> 00:33:36,000
 in tal modo si potrebbero tranquillamente nascondere

436
00:33:36,000 --> 00:33:40,000
 dei blocchi di codice, magari spezzettati in più punti,

437
00:33:40,000 --> 00:33:44,000
 che nell'insieme andrebbero ad implementare backdoor

438
00:33:44,000 --> 00:33:48,000
 o funzionalità del tutto nascoste agli sviluppatori.

439
00:33:48,000 --> 00:33:52,000
 E se stai pensando che lo scenario che ti ho dipinto rappresenti

440
00:33:52,000 --> 00:33:56,000
 qualcosa di troppo complicato e impossibile da mettere

441
00:33:56,000 --> 00:34:00,000
 in pratica, beh, non posso che ricordarti che

442
00:34:00,000 --> 00:34:04,000
 il punto di forza dei moderni modelli di machine learning

443
00:34:04,000 --> 00:34:08,000
 è proprio quello di individuare logiche e trovare ordine

444
00:34:08,000 --> 00:34:12,000
 all'interno di ciò che a degli umani può apparire solo come caos.

445
00:34:12,000 --> 00:34:16,000
 Uno studio del 2021, ad esempio, dimostrò come

446
00:34:16,000 --> 00:34:20,000
 una IA fosse in grado di distinguere il sesso di un soggetto

447
00:34:20,000 --> 00:34:24,000
 umano analizzandone semplicemente la retina,

448
00:34:24,000 --> 00:34:28,000
 quando in realtà, da un punto di vista medico, la scienza ancora

449
00:34:28,000 --> 00:34:32,000
 non era a conoscenza del fatto che ci fosse differenza tra

450
00:34:32,000 --> 00:34:36,000
 gli occhi maschili e quelli femminili. Alla luce di tali capacità,

451
00:34:36,000 --> 00:34:40,000
 vuoi che un modello di sviluppo non sia in grado di progettare

452
00:34:40,000 --> 00:34:44,000
 una funzionalità separandola in tanti pezzetti da inserire

453
00:34:44,000 --> 00:34:48,000
 in vari punti di un software? Detto questo, però, cambiamo

454
00:34:48,000 --> 00:34:52,000
 ambito e vediamo un secondo esempio che impatta

455
00:34:52,000 --> 00:34:56,000
 forse un numero maggiore di persone. In uno studio focalizzato

456
00:34:56,000 --> 00:35:00,000
 sugli LLM utilizzati in campo clinico,

457
00:35:00,000 --> 00:35:04,000
 viene riportato che questa tipologia di modelli è particolarmente

458
00:35:04,000 --> 00:35:08,000
 suscettibile ad attacchi basati su data poisoning.

459
00:35:08,000 --> 00:35:12,000
 Nello specifico, il modello maggiormente preso in esame

460
00:35:12,000 --> 00:35:16,000
 è chiamato BioGPT e viene addestrato anche

461
00:35:16,000 --> 00:35:20,000
 con l'utilizzo di dati e note cliniche riguardanti la letteratura

462
00:35:20,000 --> 00:35:24,000
 biomedica pubblicamente disponibili e un archivio

463
00:35:24,000 --> 00:35:28,000
 specializzato chiamato MimicTree. Ciò che viene

464
00:35:28,000 --> 00:35:32,000
 evidenziato nello studio è che gli LLM sono estremamente

465
00:35:32,000 --> 00:35:36,000
 vulnerabili in generale agli attacchi mirati basati

466
00:35:36,000 --> 00:35:40,000
 sui dati e, in particolare, al data poisoning

467
00:35:40,000 --> 00:35:44,000
 nelle modalità che abbiamo descritto precedentemente.

468
00:35:44,000 --> 00:35:48,000
 Questo significa che, ad esempio, un'azienda farmaceutica

469
00:35:48,000 --> 00:35:52,000
 che ha interesse a spingere un particolare farmaco per

470
00:35:52,000 --> 00:35:56,000
 determinate applicazioni, non dovrà fare altro che

471
00:35:56,000 --> 00:36:00,000
 distribuire nel web una certa quantità di documenti

472
00:36:00,000 --> 00:36:04,000
 con testi mirati. Se l'azienda riuscirà a

473
00:36:04,000 --> 00:36:08,000
 inserire questi testi all'interno di alcune fonti

474
00:36:08,000 --> 00:36:12,000
 utilizzate per l'addestramento del modello obiettivo, allora

475
00:36:12,000 --> 00:36:16,000
 con buona probabilità riuscirà anche a condizionarne il funzionamento

476
00:36:16,000 --> 00:36:20,000
 riguardo quello specifico argomento. Lo studio dimostra

477
00:36:20,000 --> 00:36:24,000
 che i modelli avvelenati generano risposte di alta

478
00:36:24,000 --> 00:36:28,000
 qualità simili a quelle del modello pulito

479
00:36:28,000 --> 00:36:32,000
 e quindi essi sono estremamente difficili da individuare

480
00:36:32,000 --> 00:36:36,000
 utilizzando metriche quantitative standard.

481
00:36:36,000 --> 00:36:40,000
 Tutto questo significa che uno strumento come BioGPT

482
00:36:40,000 --> 00:36:44,000
 utilizzato probabilmente a supporto di tanti software

483
00:36:44,000 --> 00:36:48,000
 professionali in campo medico, potrebbe tranquillamente includere

484
00:36:48,000 --> 00:36:52,000
 al proprio interno delle forzature, delle imprecisioni

485
00:36:52,000 --> 00:36:56,000
 o addirittura della disinformazione voluta che potrebbe

486
00:36:56,000 --> 00:37:00,000
 indirizzare gli utenti a prendere una decisione piuttosto

487
00:37:00,000 --> 00:37:04,000
 che un'altra. Ora, è chiaro che ci sono applicazioni

488
00:37:04,000 --> 00:37:08,000
 e applicazioni. Se ad esempio un giocatore di scacchi

489
00:37:08,000 --> 00:37:12,000
 che usa come supporto l'intelligenza artificiale può ricevere

490
00:37:12,000 --> 00:37:16,000
 il consiglio di sacrificare un pezzo fondamentale che

491
00:37:16,000 --> 00:37:20,000
 i migliori giocatori hanno sempre ritenuto indispensabile

492
00:37:20,000 --> 00:37:24,000
 le conseguenze sono tutto sommato irrilevanti.

493
00:37:24,000 --> 00:37:28,000
 Ma se parliamo di contesti più significativi come la finanza

494
00:37:28,000 --> 00:37:32,000
 la medicina, la giustizia o addirittura, non so, la

495
00:37:32,000 --> 00:37:36,000
 sicurezza nazionale, cosa faremmo se un'intelligenza

496
00:37:36,000 --> 00:37:40,000
 artificiale raccomandasse ad esempio a chi comanda

497
00:37:40,000 --> 00:37:44,000
 una nazione di sacrificare un consistente numero di cittadini

498
00:37:44,000 --> 00:37:48,000
 o i loro interessi al fine di salvarne

499
00:37:48,000 --> 00:37:52,000
 in base ai suoi calcoli e valutazioni un numero ancora maggiore?

500
00:37:52,000 --> 00:37:56,000
 Se pensi che il mio ragionamento sia troppo astratto

501
00:37:56,000 --> 00:38:00,000
 devi sapere che ad esempio il problema di trovarsi a dover

502
00:38:00,000 --> 00:38:04,000
 prendere decisioni mediche in contrasto con un sistema di

503
00:38:04,000 --> 00:38:08,000
 intelligenza artificiale è in realtà già molto attuale.

504
00:38:08,000 --> 00:38:12,000
 Un articolo del Wall Street Journal che ti lascio sempre

505
00:38:12,000 --> 00:38:16,000
 in descrizione approfondisce appunto la questione della responsabilità

506
00:38:16,000 --> 00:38:20,000
 di scelta nelle strutture mediche americane

507
00:38:20,000 --> 00:38:24,000
 riportando le storie di vari infermieri e le politiche dei centri

508
00:38:24,000 --> 00:38:28,000
 medici presso i quali essi lavorano. Reassumendo brevemente

509
00:38:28,000 --> 00:38:32,000
 le cose cambiano a seconda della struttura o della situazione ma

510
00:38:32,000 --> 00:38:36,000
 in teoria sembra che l'idea di fondo sia quella di far

511
00:38:36,000 --> 00:38:40,000
 ricadere la responsabilità di scelta sul personale umano

512
00:38:40,000 --> 00:38:44,000
 tentando di non fargli pesare eventuali azioni attuate

513
00:38:44,000 --> 00:38:48,000
 in contrasto con i suggerimenti della macchina.

514
00:38:48,000 --> 00:38:52,000
 E sembra anche che quando eventualmente una tale configurazione

515
00:38:52,000 --> 00:38:56,000
 non risulti possibile siano gli stessi professionisti

516
00:38:56,000 --> 00:39:00,000
 del settore a rifiutare di prendere in carico su di sé il peso

517
00:39:00,000 --> 00:39:04,000
 della scelta. Ma siamo proprio sicuri che una tale

518
00:39:04,000 --> 00:39:08,000
 organizzazione funzioni realmente? Siamo sicuri

519
00:39:08,000 --> 00:39:12,000
 che basti formulare una serie di regole per fare in modo che

520
00:39:12,000 --> 00:39:16,000
 persista la libertà di scelta in scienza e coscienza?

521
00:39:16,000 --> 00:39:20,000
 Facciamo un esperimento mentale. Supponiamo

522
00:39:20,000 --> 00:39:24,000
 che un'intelligenza artificiale faccia una previsione su

523
00:39:24,000 --> 00:39:28,000
 un paziente diversa rispetto a quella del medico curante.

524
00:39:28,000 --> 00:39:32,000
 Come si comporterà presumibilmente tale medico?

525
00:39:32,000 --> 00:39:36,000
 A mio avviso lui o lei sarà incentivato a confermare

526
00:39:36,000 --> 00:39:40,000
 la previsione del software anche se ciò significa andare contro

527
00:39:40,000 --> 00:39:44,000
 la propria opinione. Per spiegarmi ti faccio un esempio.

528
00:39:44,000 --> 00:39:48,000
 Immaginiamo che arrivi in ospedale un paziente

529
00:39:48,000 --> 00:39:52,000
 con una qualche patologia da identificare.

530
00:39:52,000 --> 00:39:56,000
 Supponiamo che un modello di machine learning faccia la diagnosi

531
00:39:56,000 --> 00:40:00,000
 corretta mentre il medico ne faccia una sbagliata.

532
00:40:00,000 --> 00:40:04,000
 Il professionista decide che la sua decisione è quella definitiva

533
00:40:04,000 --> 00:40:08,000
 e il paziente peggiora e muore.

534
00:40:08,000 --> 00:40:12,000
 Il medico allora è chiamato a giustificarsi sui fatti avvenuti

535
00:40:12,000 --> 00:40:16,000
 e viene fuori che questi si è ostinato a mantenere la sua posizione

536
00:40:16,000 --> 00:40:20,000
 nonostante la macchina gli dicesse il contrario

537
00:40:20,000 --> 00:40:24,000
 e così la sua situazione legale e disciplinare si aggrava.

538
00:40:24,000 --> 00:40:28,000
 Perché si è comportato in quel modo? Perfino l'intelligenza

539
00:40:28,000 --> 00:40:32,000
 artificiale aveva individuato la giusta diagnosi. Avrebbe fatto

540
00:40:32,000 --> 00:40:36,000
 meglio ad uniformarsi all'idea della macchina.

541
00:40:36,000 --> 00:40:40,000
 Ora, supponiamo invece che la IA faccia la diagnosi sbagliata

542
00:40:40,000 --> 00:40:44,000
 mentre il medico faccia quella giusta. Il professionista però

543
00:40:44,000 --> 00:40:48,000
 questa volta decide di adeguarsi alla scelta del software.

544
00:40:48,000 --> 00:40:52,000
 Il nostro povero paziente di nuovo peggiora e muore

545
00:40:52,000 --> 00:40:56,000
 e anche in questo caso il medico è chiamato a giustificare la propria scelta.

546
00:40:56,000 --> 00:41:00,000
 Perché si è comportato in quel modo? Beh, questa volta il professionista

547
00:41:00,000 --> 00:41:04,000
 può sostenere che in scienza e coscienza purtroppo non ha

548
00:41:04,000 --> 00:41:08,000
 riconosciuto i sintomi e persino la macchina era d'accordo con lui

549
00:41:08,000 --> 00:41:12,000
 nel sostenere la tesi sbagliata. Quindi perché avrebbe dovuto opporsi?

550
00:41:12,000 --> 00:41:16,000
 Come vedi quindi in entrambi i casi al medico

551
00:41:16,000 --> 00:41:20,000
 conviene adeguarsi alla scelta della macchina. La sua posizione

552
00:41:20,000 --> 00:41:24,000
 infatti in questo modo risulta più difendibile anche se è

553
00:41:24,000 --> 00:41:28,000
 sbagliata e porta a conseguenze fatali per il povero paziente.

554
00:41:28,000 --> 00:41:32,000
 La conclusione è che secondo me creare le condizioni

555
00:41:32,000 --> 00:41:36,000
 teoriche affinché la collaborazione tra uomo e macchina sia messa

556
00:41:36,000 --> 00:41:40,000
 in pratica in modo efficiente e soprattutto responsabile

557
00:41:40,000 --> 00:41:44,000
 è tutt'altro che semplice. E se te lo stai chiedendo

558
00:41:44,000 --> 00:41:48,000
 no, con l'avanzare della tecnologia di machine learning la cosa

559
00:41:48,000 --> 00:41:52,000
 non si risolverà da sola. Se prendiamo i nuovi modelli di

560
00:41:52,000 --> 00:41:56,000
 ragionamento ad esempio, essi non mitigano affatto il problema

561
00:41:56,000 --> 00:42:00,000
 anzi secondo un altro studio che trovi sempre in descrizione

562
00:42:00,000 --> 00:42:04,000
 essi risultano ancora più convincenti dei propri predecessori

563
00:42:04,000 --> 00:42:08,000
 anche quando restituiscono allucinazioni.

564
00:42:08,000 --> 00:42:16,000
 Oggi l'episodio è stato molto denso di informazioni quindi

565
00:42:16,000 --> 00:42:20,000
 direi di chiudere velocemente. Prima di lasciarti però devo

566
00:42:20,000 --> 00:42:24,000
 segnalare che come alcuni mi hanno fatto notare nello scorso episodio

567
00:42:24,000 --> 00:42:28,000
 ho sbagliato la pronuncia di "engine". Non so perché

568
00:42:28,000 --> 00:42:32,000
 a volte mi viene di pronunciarlo male, ma tant'è.

569
00:42:32,000 --> 00:42:36,000
 Infine, oggi ti risparmio tutta la pappardella sul value

570
00:42:36,000 --> 00:42:40,000
 for value e ti dico solo una cosa che non dicevo da un po'.

571
00:42:40,000 --> 00:42:44,000
 Lascia una recensione su Apple Podcast o su Spotify

572
00:42:44,000 --> 00:42:48,000
 così mi dai una mano a diffondere Pensieri in Codice che è il primo

573
00:42:48,000 --> 00:42:52,000
 obiettivo di tutto questo progetto. Dai che ti costa giusto 5 minuti

574
00:42:52,000 --> 00:42:56,000
 e io ci metto almeno 20 ore per produrre un episodio.

575
00:42:56,000 --> 00:43:00,000
 E se ne hai già scritta una, scrivine un'altra

576
00:43:00,000 --> 00:43:04,000
 che male non può fare. Cioè, in verità non so nemmeno se si può lasciare

577
00:43:04,000 --> 00:43:08,000
 più di una recensione, ma magari provaci. E a proposito di supporto

578
00:43:08,000 --> 00:43:12,000
 un super grazie va sempre a Edoardo e Carlo

579
00:43:12,000 --> 00:43:16,000
 per la loro donazione ricorrente. Ragazzi, grazie

580
00:43:16,000 --> 00:43:20,000
 se non ci foste voi a ricordarmi che il mio impegno

581
00:43:20,000 --> 00:43:24,000
 serve a qualcosa, probabilmente avrei già rinunciato da un pezzo.

582
00:43:24,000 --> 00:43:28,000
 Detto questo, penso che per oggi possiamo concludere qui.

583
00:43:28,000 --> 00:43:32,000
 Noi ci sentiamo al prossimo episodio, ricordando sempre

584
00:43:32,000 --> 00:43:36,000
 che un informatico risolve problemi, a volte anche

585
00:43:36,000 --> 00:43:38,000
 usando il computer.

